@web_page{,
   title = {Amazon EC2 Pricing - Amazon Web Services},
   url = {https://aws.amazon.com/ec2/pricing/},
}
@article{Hahne2020,
   abstract = {In this paper, we introduce the Python framework PyMGRIT, which implements the multigrid-reduction-in-time (MGRIT) algorithm for solving the (non-)linear systems arising from the discretization of time-dependent problems. The MGRIT algorithm is a reduction-based iterative method that allows parallel-in-time simulations, i. e., calculating multiple time steps simultaneously in a simulation, by using a time-grid hierarchy. The PyMGRIT framework features many different variants of the MGRIT algorithm, ranging from different multigrid cycle types and relaxation schemes, as well as various coarsening strategies, including time-only and space-time coarsening, to using different time integrators on different levels in the multigrid hierachy. PyMGRIT allows serial runs for prototyping and testing of new approaches, as well as parallel runs using the Message Passing Interface (MPI). Here, we describe the implementation of the MGRIT algorithm in PyMGRIT and present the usage from both user and developer point of views. Three examples illustrate different aspects of the package, including pure time parallelism as well as space-time parallelism by coupling PyMGRIT with PETSc or Firedrake, which enable spatial parallelism through MPI.},
   author = {Jens Hahne and Stephanie Friedhoff and Matthias Bolten},
   keywords = {Additional Key Words and Phrases: Multigrid-reduction-in-time (MGRIT), parallel-in-time integration,CCS Concepts: •Mathematics of computing → Solvers,Diierential algebraic equations,Ordinary diierential equations,Partial diierential equations},
   month = {8},
   title = {PyMGRIT: A Python Package for the parallel-in-time method MGRIT},
   url = {http://arxiv.org/abs/2008.05172},
   year = {2020},
}
@web_page{,
   title = {NVIDIA V100 | NVIDIA},
   url = {https://www.nvidia.com/en-us/data-center/v100/},
}
@web_page{,
   title = {GeForce GTX 1080 Ti Graphics Cards | NVIDIA GeForce},
   url = {https://www.nvidia.com/en-sg/geforce/products/10series/geforce-gtx-1080-ti/},
}
@web_page{,
   title = {Intel® Xeon® Processor E5-2698 v4 91753},
   url = {https://www.intel.com/content/www/us/en/products/processors/xeon/e5-processors/e5-2698-v4.html},
}
@web_page{,
   title = {Intel® Xeon® Silver 4114 Processor 123550},
   url = {https://www.intel.com/content/www/us/en/products/processors/xeon/scalable/silver-processors/silver-4114.html},
}
@book_section{Kowarschik2003,
   abstract = {In order to mitigate the impact of the growing gap between CPU speed and main memory performance, today’s computer architectures implement hierarchical memory structures. The idea behind this approach is to hide both the low main memory bandwidth and the latency of main memory accesses which is slow in contrast to the floating-point performance of the CPUs. Usually, there is a small and expensive high speed memory sitting on top of the hierarchy which is usually integrated within the processor chip to provide data with low latency and high bandwidth; i.e., the CPU registers. Moving further away from the CPU, the layers of memory successively become larger and slower. The memory components which are located between the processor core and main memory are called cache memories or caches. They are intended to contain copies of main memory blocks to speed up accesses to frequently needed data [378], [392]. The next lower level of the memory hierarchy is the main memory which is large but also comparatively slow. While external memory such as hard disk drives or remote memory components in a distributed computing environment represent the lower end of any common hierarchical memory design, this paper focuses on optimization techniques for enhancing cache performance.},
   author = {Markus Kowarschik and Christian Weiß},
   doi = {10.1007/3-540-36574-5_10},
   pages = {213-232},
   title = {An Overview of Cache Optimization Techniques and Cache-Aware Numerical Algorithms},
   year = {2003},
}
@report{Lions2013,
   abstract = {Résolution d'EDP par un schéma en temps < pararéel >. Comptes rendus de l'Académie des sciences. Série I, Mathématique, Elsevier, 2001, 332 (7), pp.661-668. ï¿¿hal-00798372ï¿¿},
   author = {Jacques-Louis Lions and Yvon Maday and Gabriel Turinici},
   journal = {Elsevier},
   title = {Résolution d'EDP par un schéma en temps < pararéel >},
   url = {https://hal.archives-ouvertes.fr/hal-00798372},
   year = {2013},
}
@article{Maday2020,
   abstract = {In this paper, we consider the problem of accelerating the numerical simulation of time dependent problems by time domain decomposition. The available algorithms enabling such decompositions present severe efficiency limitations and are an obstacle for the solution of large scale and high dimensional problems. Our main contribution is the improvement of the parallel efficiency of the parareal in time method. The parareal method is based on combining predictions made by a numerically inexpensive solver (with coarse physics and/or coarse resolution) with corrections coming from an expensive solver (with high-fidelity physics and high resolution). At convergence, the algorithm provides a solution that has the fine solver's high-fidelity physics and high resolution. In the classical version, the fine solver has a fixed high accuracy which is the major obstacle to achieve a competitive parallel efficiency. In this paper, we develop an adaptive variant that overcomes this obstacle by dynamically increasing the accuracy of the fine solver across the parareal iterations. We theoretically show that the parallel efficiency becomes very competitive in the ideal case where the cost of the coarse solver is small, thus proving that the only remaining factors impeding full scalability become the cost of the coarse solver and communication time. The developed theory has also the merit of setting a general framework to understand the success of several extensions of parareal based on iteratively improving the quality of the fine solver and re-using information from previous parareal steps. We illustrate the actual performance of the method in stiff ODEs, which are a challenging family of problems since the only mechanism for adaptivity is time and efficiency is affected by the cost of the coarse solver.},
   author = {Y. Maday and O. Mula},
   doi = {10.1016/j.cam.2020.112915},
   issn = {03770427},
   journal = {Journal of Computational and Applied Mathematics},
   keywords = {Convergence rates,Domain decomposition,Inexact fine solver,Parallel efficiency,Parareal in time algorithm,a posteriori estimators},
   month = {10},
   publisher = {Elsevier B.V.},
   title = {An adaptive parareal algorithm},
   volume = {377},
   url = {https://plmlab.math.cnrs.fr/mulahernandez/parareal-adaptive},
   year = {2020},
}
@report{Gander2014,
   abstract = {We present and analyze a new space-time parallel multigrid method for parabolic equations. The method is based on arbitrarily high order discontinuous Galerkin discretizations in time, and a finite element discretization in space. The key ingredient of the new algorithm is a block Jacobi smoother. We present a detailed convergence analysis when the algorithm is applied to the heat equation, and determine asymptotically optimal smoothing parameters, a precise criterion for semi-coarsening in time or full coarsening, and give an asymptotic two grid contraction factor estimate. We then explain how to implement the new multigrid algorithm in parallel, and show with numerical experiments its excellent strong and weak scalability properties. Key words. Space-time parallel methods, multigrid in space-time, DG-discretizations, strong and weak scalability, parabolic problems AMS subject classifications. 65N55, 65F10, 65L60 1. Introduction. About ten years ago, clock speeds of processors have stopped increasing, and the only way to obtain more performance is by using more processing cores. This has led to new generations of supercomputers with millions of computing cores, and even today's small devices are multicore. In order to exploit these new architectures for high performance computing, algorithms must be developed that can use these large numbers of cores efficiently. When solving evolution partial differential equations, the time direction offers itself as a further direction for parallelization, in addition to the spatial directions, and the parareal algorithm [29, 31, 1, 37, 18, 9] has sparked renewed interest in the area of time parallelization, a field that is now just over fifty years old, see the historical overview [8]. We are interested here in space-time parallel methods, which can be based on the two fundamental paradigms of domain decomposition or multigrid. Domain decomposition methods in space-time lead to waveform relaxation type methods, see [17, 7, 19] for classical Schwarz waveform relaxation, [12, 13, 10, 11, 2] for optimal and optimized variants, and [28, 33, 15] for Dirichlet-Neumann and Neumann-Neumann waveform relaxation. The spatial decompositions can be combined with parareal to obtain algorithms that run on arbitrary decompositions of the space-time domain into space-time subdomains, see [32, 14]. Space-time multigrid methods were developed in [20, 30, 41, 25, 40, 26, 27, 43], and reached good F-cycle convergence behavior when appropriate semi-coarsening and extension operators are used. For a variant for non-linear problems, see [4, 36, 35]. We present and analyze here a new space-time parallel multigrid algorithm that has excellent strong and weak scalability properties on large scale parallel computers. As a model problem we consider the heat equation in a bounded domain Ω ⊂ R d , d = 1, 2, 3 with boundary Γ := ∂Ω on the bounded time interval [0, T ],},
   author = {Martin J Gander and Martin Neum¨uller and Neum¨ Neum¨uller},
   journal = {SIAM},
   title = {ANALYSIS OF A NEW SPACE-TIME PARALLEL MULTIGRID ALGORITHM FOR PARABOLIC PROBLEMS},
   url = {https://epubs.siam.org/doi/abs/10.1137/15M1046605},
   year = {2014},
}
@report{,
   abstract = {A novel parallel algorithm for the integration of linear initial-value problems is proposed. This algorithm is based on the simple observation that homogeneous problems can typically be integrated much faster than inhomogeneous problems. An overlapping time-domain decomposition is utilized to obtain decoupled inhomogeneous and homogeneous subproblems, and a near-optimal Krylov method is used for the fast exponential integration of the homogeneous subproblems. We present an error analysis and discuss the parallel scaling of our algorithm. The efficiency of this approach is demonstrated with numerical examples.},
   author = {Martin J Gander and Stefan G ¨ Uttel},
   isbn = {200020131826/1},
   journal = {SIAM},
   keywords = {65F60,65Y05,linear initial-value problem,matrix exponential AMS subject classifications 65L05,parallelization,rational Krylov},
   title = {PARAEXP: A PARALLEL INTEGRATOR FOR LINEAR INITIAL-VALUE PROBLEMS *},
   url = {https://epubs.siam.org/doi/abs/10.1137/110856137},
}
@report{,
   abstract = {The parallel full approximation scheme in space and time (PFASST) introduced by Emmett and Minion in 2012 is an iterative strategy for the temporal parallelization of ODEs and discretized PDEs. As the name suggests, PFASST is similar in spirit to a space-time Full Approximation Scheme (FAS) multigrid method performed over multiple time-steps in parallel. However, since the original focus of PFASST has been on the performance of the method in terms of time par-allelism, the solution of any spatial system arising from the use of implicit or semi-implicit temporal methods within PFASST have simply been assumed to be solved to some desired accuracy completely at each sub-step and each iteration by some unspecified procedure. It hence is natural to investigate how iterative solvers in the spatial dimensions can be interwoven with the PFASST iterations and whether this strategy leads to a more efficient overall approach. This paper presents an initial investigation on the relative performance of different strategies for coupling PFASST iterations with multigrid methods for the implicit treatment of diffusion terms in PDEs. In particular, we compare full accuracy multigrid solves at each sub-step with a small fixed number of multigrid V-cycles. This reduces the cost of each PFASST iteration at the possible expense of a corresponding increase in the number of PFASST iterations needed for convergence. Parallel efficiency of the resulting methods is explored through numerical examples. Key words. Parallel in time, PFASST, multigrid 1. Introduction. The past decade has seen a growing interest in the development of parallel methods for temporal integration of ordinary differential equations (ODEs), particularly in the context of temporal strategies for partial differential equations (PDEs). One factor fueling this interest is related to the evolution of supercom-puters during this time. Since the end of the exponential increase in individual processors speeds, increases in supercomputer speeds have been mostly due to increases in the number of computational cores, and current projections suggest that the first exaflop computer will contain on the order of a billion cores [14]. The implication of this trend is that increasing concurrency in algorithms is essential, and in the case of time-dependent PDE simulations, the use of space-time parallelism is an attractive option. Time-parallel methods have a long history dating back at least to the work of Nievergelt [26]. In the context of space-time multigrid, Hackbusch noted already in 1984 that relaxation operators in parabolic multigrid can be employed on multiple time steps simultaneously [15]. The 1997 review article by Burrage [7] provides a summary of early work on the subject. More recently, the parareal method proposed in 2001 [22] has renewed interest in temporal parallelization methods. In 2012 the parallel full approximation scheme in space and time (PFASST) was introduced by Emmett and Minion [9], and performance results for PFASST using space-time par-allelization with hundreds of thousands of cores can be found in [31, 28]. The PFASST algorithm is based on a type of deferred corrections strategy for ODEs [8], with corrections being applied on multiple time steps in parallel. As such, there are similarities between parareal and PFASST (see [25, 24]). On the other hand, the parallel efficiency of PFASST depends on the construction of a hierarchy of space-time discretizations, hence there are also similarities between PFASST and space},
   author = {M L Minion and R Speck and M Bolten and M Emmett and D Ruprecht},
   journal = {SIAM},
   title = {INTERWEAVING PFASST AND PARALLEL MULTIGRID},
   url = {https://epubs.siam.org/doi/abs/10.1137/14097536X},
}
@article{,
   author = {M Emmett and M Minion - in Applied Mathematics and Computational Science and undefined 2012},
   journal = {msp.org},
   title = {Toward an efficient parallel in time method for partial differential equations},
   url = {https://msp.org/camcos/2012/7-1/p04.xhtml},
}
@article{Wu2018,
   abstract = {It is challenge work to design parareal algorithms for time-fractional differential equations due to the historical effect of the fractional operator. A direct extension of the classical parareal method to such equations will lead to unbalance computational time in each process. In this work, we present an efficient parareal iteration scheme to overcome this issue, by adopting two recently developed local time-integrators for time fractional operators. In both approaches, one introduces auxiliary variables to localized the fractional operator. To this end, we propose a new strategy to perform the coarse grid correction so that the auxiliary variables and the solution variable are corrected separately in a mixed pattern. It is shown that the proposed parareal algorithm admits robust rate of convergence. Numerical examples are presented to support our conclusions.},
   author = {Shu-Lin Wu and Tao Zhou},
   doi = {10.1016/j.jcp.2017.12.029},
   journal = {Article in Journal of Computational Physics},
   keywords = {Local time-integrators,Parareal,Time-fractional differential equations},
   pages = {135-149},
   title = {Parareal algorithms with local time-integrators for time fractional differential equations Fast Computation Algorithms for Differential Equations View project Numerical methods for time and space fractional partial differential equations View project Parareal algorithms with local time-integrators for time fractional differential equations ✩},
   volume = {358},
   url = {www.elsevier.com/locate/jcp},
   year = {2018},
}
@article{Falgout2017,
   abstract = {The need for parallelism in the time dimension is being driven by changes in computer architectures, where performance increases are now provided through greater concurrency, not faster clock speeds. This creates a bottleneck for sequential time marching schemes because they lack parallelism in the time dimension. Multigrid reduction in time (MGRIT) is an iterative procedure that allows for temporal parallelism by utilizing multigrid reduction techniques and a multilevel hierarchy of coarse time grids. MGRIT has been shown to be effective for linear problems, with speedups of up to 50 times. The goal of this work is the eficient solution of nonlinear problems with MGRIT, where eficiency is defined as achieving similar performance when compared to an equivalent linear problem. The benchmark nonlinear problem is the p-Laplacian, where p = 4 corresponds to a well-known nonlinear diffusion equation and p = 2 corresponds to the standard linear diffusion operator, our benchmark linear problem. The key dificulty encountered is that the nonlinear timestep solver becomes progressively more expensive on coarser time levels as the time-step size increases. To overcome such dificulties, multigrid research has historically targeted an accumulated body of experience regarding how to choose an appropriate solver for a specific problem type. To that end, this paper develops a library of MGRIT optimizations and modifications, most important an alternate initial guess for the nonlinear time-step solver and delayed spatial coarsening, that will allow many nonlinear parabolic problems to be solved with parallel scaling behavior comparable to the corresponding linear problem.},
   author = {R. D. Falgout and T. A. Manteuffel and B. O'Neill and J. B. Schroder},
   doi = {10.1137/16M1082330},
   issn = {10957197},
   issue = {5},
   journal = {SIAM Journal on Scientific Computing},
   keywords = {High performance computing,Multigrid,Multigrid-in-time,Nonlinear,Parabolic problems,Parareal,Reduction-based multigrid},
   pages = {S298-S322},
   publisher = {Society for Industrial and Applied Mathematics Publications},
   title = {Multigrid reduction in time for nonlinear parabolic problems: A case study},
   volume = {39},
   year = {2017},
}
@article{Falgout2014,
   abstract = {We consider optimal-scaling multigrid solvers for the linear systems that arise from the discretization of problems with evolutionary behavior. Typically, solution algorithms for evolution equations are based on a time-marching approach, solving sequentially for one time step after the other. Parallelism in these traditional time-integration techniques is limited to spatial parallelism. However, current trends in computer architectures are leading toward systems with more, but not faster, processors. Therefore, faster compute speeds must come from greater parallelism. One approach to achieving parallelism in time is with multigrid, but extending classical multigrid methods for elliptic operators to this setting is not straightforward. In this paper, we present a nonintrusive, optimal-scaling time-parallel method based on multigrid reduction (MGR). We demonstrate optimality of our multigrid-reduction-in-time algorithm (MGRIT) for solving diffusion equations in two and three space dimensions in numerical experiments. Furthermore, through both parallel performance models and actual parallel numerical results, we show that we can achieve significant speedup in comparison to sequential time marching on modern architectures.},
   author = {R. D. Falgout and S. Friedhoff and Tz V. Kolev and S. P. MacLachlan and J. B. Schroder},
   doi = {10.1137/130944230},
   issn = {10957200},
   issue = {6},
   journal = {SIAM Journal on Scientific Computing},
   keywords = {Multigrid-in-time,Parabolic problems,Parareal,Reduction-based multigrid},
   pages = {C635-C661},
   publisher = {Society for Industrial and Applied Mathematics Publications},
   title = {Parallel time integration with multigrid},
   volume = {36},
   year = {2014},
}
@report{,
   abstract = {Time parallel time integration methods have received renewed interest over the last decade because of the advent of massively parallel computers, which is mainly due to the clock speed limit reached on today's processors. When solving time dependent partial differential equations, the time direction is usually not used for parallelization. But when parallelization in space saturates, the time direction offers itself as a further direction for parallelization. The time direction is however special, and for evolution problems there is a causality principle: the solution later in time is affected (it is even determined) by the solution earlier in time, but not the other way round. Algorithms trying to use the time direction for parallelization must therefore be special, and take this very different property of the time dimension into account. We show in this chapter how time domain decomposition methods were invented, and give an overview of the existing techniques. Time parallel methods can be classified into four different groups: methods based on multiple shooting, methods based on domain decomposition and waveform relaxation, space-time multigrid methods and direct time parallel methods. We show for each of these techniques the main inventions over time by choosing specific publications and explaining the core ideas of the authors. This chapter is for people who want to quickly gain an overview of the exciting and rapidly developing area of research of time parallel methods.},
   author = {Martin J Gander},
   journal = {Springer},
   title = {50 Years of Time Parallel Time Integration},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-23321-5_3},
}
@report{,
   abstract = {Many large-scale scientific computations require eigenvalue solvers in a scaling regime where efficiency is limited by data movement. We introduce a parallel algorithm for computing the eigenvalues of a dense symmetric matrix, which performs asymptotically less communication than previously known approaches. We provide analysis in the Bulk Synchronous Parallel (BSP) model with additional consideration for communication between a local memory and cache. Given sufficient memory to store c copies of the symmetric matrix, our algorithm requires Θ(√ c) less interprocessor communication than previously known algorithms, for any c ≤ p 1/3 when using p processors. The algorithm first reduces the dense symmetric matrix to a banded matrix with the same eigenvalues. Subsequently, the algorithm employs successive reduction to O(log p) thinner banded matrices. We employ two new parallel algorithms that achieve lower communication costs for the full-to-band and band-to-band reductions. Both of these algorithms leverage a novel QR factorization algorithm for rectangular matrices.},
   author = {Edgar Solomonik and Eth Zurich and Grey Ballard and James Demmel},
   journal = {dl.acm.org},
   title = {A communication-avoiding parallel algorithm for the symmetric eigenvalue problem Torsten Hoefler},
   url = {https://dl.acm.org/doi/abs/10.1145/3087556.3087561},
}
@report{Khabou2012,
   author = {Amal Khabou and James W Demmel and Ming Gu},
   journal = {SIAM},
   title = {LU factorization with panel rank revealing pivoting and its communication avoiding version},
   url = {https://epubs.siam.org/doi/abs/10.1137/120863691},
   year = {2012},
}
@article{Ballard2011,
   abstract = {In 1981 Hong and Kung proved a lower bound on the amount of communication (amount of data moved between a small, fast memory and large, slow memory) needed to perform dense, n-by-n matrix multiplication using the conventional O(n3) algorithm, where the input matrices were too large to fit in the small, fast memory. In 2004 Irony, Toledo, and Tiskin gave a new proof of this result and extended it to the parallel case (where communication means the amount of data moved between processors). In both cases the lower bound may be expressed as ω(arithmetic operations/pM), where M is the size of the fast memory (or local memory in the parallel case). Here we generalize these results to a much wider variety of algorithms, including LU factorization, Cholesky factorization, LDLT factorization, QR factorization, the Gram-Schmidt algorithm, and algorithms for eigenvalues and singular values, i.e., essentially all direct methods of linear algebra. The proof works for dense or sparse matrices and for sequential or parallel algorithms. In addition to lower bounds on the amount of data moved (bandwidth cost), we get lower bounds on the number of messages required to move it (latency cost). We extend our lower bound technique to compositions of linear algebra operations (like computing powers of a matrix) to decide whether it is enough to call a sequence of simpler optimal algorithms (like matrix multiplication) to minimize communication, or whether we can do better. We give examples of both. We also show how to extend our lower bounds to certain graph-theoretic problems. We point out recently designed algorithms that attain many of these lower bounds. © 2011 Society for Industrial and Applied Mathematics.},
   author = {Grey Ballard and James Demmel and Olga Holtz and Oded Schwartz},
   doi = {10.1137/090769156},
   issn = {08954798},
   issue = {3},
   journal = {SIAM Journal on Matrix Analysis and Applications},
   keywords = {Bandwidth,Communication-avoiding,Latency,Linear algebra algorithms,Lower bound},
   pages = {866-901},
   title = {Minimizing communication in numerical linear algebra},
   volume = {32},
   year = {2011},
}
@book{,
   abstract = {The performance of sparse iterative solvers is typically limited by sparse matrix-vector multiplication, which is itself limited by memory system and network performance. As the gap between computation and communication speed continues to widen, these traditional sparse methods will suffer. In this paper we focus on an alternative building block for sparse iterative solvers, the "matrix powers ker-nel" [x, Ax, A 2 x,. .. , A k x], and show that by organizing computations around this kernel, we can achieve near-minimal communication costs. We consider communication very broadly as both network communication in parallel code and memory hierarchy access in sequential code. In particular, we introduce a parallel algorithm for which the number of messages (total latency cost) is independent of the power k, and a sequential algorithm, that reduces both the number and volume of accesses, so that it is independent of k in both latency and bandwidth costs. This is part of a larger project to develop "communication-avoiding Krylov subspace methods," which also addresses the numerical issues associated with these methods. Our algorithms work for general sparse matrices that "partition well". We introduce parallel performance models of matrices arising from 2D and 3D problems and show predicted speedups over a conventional algorithm of up to 7x on a Petaflop-scale machine and up to 22x on computation across the Grid. Analogous sequential performance models of the same problems predict speedups over a conventional algorithm of up to 10x on an out-of-core implementation , and up to 2.5x when we use our ideas to reduce off-chip latency and bandwidth to DRAM. Finally, we validate the model on an out-of-core sequential implementation and measured a speedup of over 3x, which is close to the predicted speedup.},
   author = {James Demmel and Mark Hoemmen and Marghoob Mohiyuddin and Katherine Yelick},
   isbn = {9781424416943},
   journal = {ieeexplore.ieee.org},
   title = {Avoiding Communication in Sparse Matrix Computations},
   url = {https://ieeexplore.ieee.org/abstract/document/4536305/},
}
@article{,
   author = {M Baboulin and S Donfack and J Dongarra and L Grigori - Procedia Computer … and undefined 2012},
   journal = {Elsevier},
   title = {A class of communication-avoiding algorithms for solving general dense linear systems on CPU/GPU parallel machines},
   url = {https://www.sciencedirect.com/science/article/pii/S187705091200124X},
}
@book_section{Gander2015,
   abstract = {Time parallel time integration methods have received renewed interest over the last decade because of the advent of massively parallel computers, which is mainly due to the clock speed limit reached on today's processors. When solving time dependent partial differential equations, the time direction is usually not used for parallelization. But when parallelization in space saturates, the time direction offers itself as a further direction for parallelization. The time direction is however special, and for evolution problems there is a causality principle: the solution later in time is affected (it is even determined) by the solution earlier in time, but not the other way round. Algorithms trying to use the time direction for parallelization must therefore be special, and take this very different property of the time dimension into account. We show in this chapter how time domain decomposition methods were invented, and give an overview of the existing techniques. Time parallel methods can be clas-sified into four different groups: methods based on multiple shooting, methods based on domain decomposition and waveform relaxation, space-time multigrid methods and direct time parallel methods. We show for each of these techniques the main inventions over time by choosing specific publications and explaining the core ideas of the authors. This chapter is for people who want to quickly gain an overview of the exciting and rapidly developing area of research of time parallel methods.},
   author = {Martin J. Gander},
   doi = {10.1007/978-3-319-23321-5_3},
   pages = {69-113},
   title = {50 Years of Time Parallel Time Integration},
   year = {2015},
}
@article{Ghorpade2012,
   abstract = {The future of computation is the Graphical Processing Unit, i.e. the GPU. The promise that the graphics cards have shown in the field of image processing and accelerated rendering of 3D scenes, and the computational capability that these GPUs possess, they are developing into great parallel computing units. It is quite simple to program a graphics processor to perform general parallel tasks. But after understanding the various architectural aspects of the graphics processor, it can be used to perform other taxing tasks as well. In this paper, we will show how CUDA can fully utilize the tremendous power of these GPUs. CUDA is NVIDIA's parallel computing architecture. It enables dramatic increases in computing performance, by harnessing the power of the GPU. This paper talks about CUDA and its architecture. It takes us through a comparison of CUDA C/C++ with other parallel programming languages like OpenCL and DirectCompute. The paper also lists out the common myths about CUDA and how the future seems to be promising for CUDA.},
   author = {Jayshree Ghorpade and Jitendra Parande and Madhura Kulkarni and Amit Bawaskar},
   doi = {10.5121/acij.2012.3109},
   issue = {1},
   journal = {Advanced Computing: An International Journal ( ACIJ )},
   keywords = {ALU,CUDA,DirectCompute,GFLOPS,GPGPU,GPU,OpenCL,block,data parallelism,grid,thread},
   title = {GPGPU PROCESSING IN CUDA ARCHITECTURE},
   volume = {3},
   year = {2012},
}
@generic{Alexandrov2016,
   abstract = {This editorial outlines the research context, the needs and challenges on the route to exascale. In particular the focus is on novel mathematical methods and mathematical modeling approaches together with scalable scientific algorithms that are needed to enable key science applications at extreme-scale. This is especially true as HPC systems continue to scale up in compute node and processor core count. These extreme-scale systems require novel mathematical methods to be developed that lead to scalable scientific algorithms to hide network and memory latency, have very high computation/communication overlap, have minimal communication, have fewer synchronization points. It stresses the need of scalability at all levels, starting from mathematical methods level through algorithmic level, and down to systems level in order to achieve overall scalability. It also points out that with the advances of Data Science in the past few years the need of such scalable mathematical methods and algorithms able to handle data and compute intensive applications at scale becomes even more important. The papers in the special issue are selected to address one or several key challenges on the route to exascale.},
   author = {Vassil Alexandrov},
   doi = {10.1016/j.jocs.2016.04.014},
   issn = {18777503},
   journal = {Journal of Computational Science},
   keywords = {Computational Science research methods,Exascale computing,HPC,Novel mathematical methods,Scalable algorithms},
   month = {5},
   pages = {1-4},
   publisher = {Elsevier B.V.},
   title = {Route to exascale: Novel mathematical methods, scalable algorithms and Computational Science skills},
   volume = {14},
   url = {http://dx.doi.org/10.1016/j.jocs.2016.04.014},
   year = {2016},
}
@article{Collette2008,
   author = {A Collette},
   title = {HDF5 for Python},
   year = {2008},
}
@report{Leveque2002,
   author = {Randall J Leveque},
   isbn = {0521810876},
   title = {Finite Volume Methods for Hyperbolic Problems},
   url = {http://www.cambridge.org},
   year = {2002},
}
@article{Magee2020,
   abstract = {Applications that exploit the architectural details of high-performance computing (HPC) systems have become increasingly invaluable in academia and industry over the past two decades. The most important hardware development of the last decade in HPC has been the general purpose graphics processing unit (GPGPU), a class of massively parallel devices that now contributes the majority of computational power in the top 500 supercomputers. As these systems grow, small costs such as latency—due to the fixed cost of memory accesses and communication—accumulate in a large simulation and become a significant barrier to performance. The swept time-space decomposition rule is a communication-avoiding technique for time-stepping stencil update formulas that attempts to reduce latency costs. This work extends the swept rule by targeting heterogeneous, CPU/GPU architectures representing current and future HPC systems. We compare our approach to a naive decomposition scheme with two test equations using an MPI+CUDA pattern on 40 processes over two nodes containing one GPU. The swept rule produces a factor of 1.9 to 23 speedup for the heat equation and a factor of 1.1 to 2.0 speedup for the Euler equations, using the same processors and work distribution, and with the best possible configurations. These results show the potential effectiveness of the swept rule for different equations and numerical schemes on massively parallel compute systems that incur substantial latency costs.},
   author = {Daniel J. Magee and Anthony S. Walker and Kyle E. Niemeyer},
   doi = {10.1007/s11227-020-03340-9},
   issn = {15730484},
   journal = {Journal of Supercomputing},
   keywords = {Communication-avoiding algorithms,Computational fluid dynamics,Domain decomposition,Heterogeneous computing,Partial differential equations},
   publisher = {Springer},
   title = {Applying the swept rule for solving explicit partial differential equations on heterogeneous computing systems},
   year = {2020},
}
@report{,
   abstract = {MPI for Python provides bindings of the Message Passing Interface (MPI) standard for the Python programming language and allows any Python program to exploit multiple processors. This package is constructed on top of the MPI-1 specification and defines an object oriented interface which closely follows MPI-2 C++ bindings. It supports point-to-point (sends, receives) and collective (broadcasts, scatters, gathers) communications of general Python objects. Efficiency has been tested in a Beowulf class cluster and satisfying results were obtained. MPI for Python is open source and available for download on the web (},
   author = {Lisandro Dalcín and Rodrigo Paz and Mario Storti},
   journal = {Elsevier},
   keywords = {High level languages,MPI,Message passing,Parallel Python},
   title = {MPI for Python},
   url = {http://www.cimec.org.ar/python},
}
@article{Hoefler2013,
   abstract = {Hybrid parallel programming with the message passing interface (MPI) for internode communication in conjunction with a shared-memory programming model to manage intranode parallelism has become a dominant approach to scalable parallel programming. While this model provides a great deal of flexibility and performance potential, it saddles programmers with the complexity of utilizing two parallel 123 1122 T. Hoefler et al. programming systems in the same application. We introduce an MPI-integrated shared-memory programming model that is incorporated into MPI through a small extension to the one-sided communication interface. We discuss the integration of this interface with the MPI 3.0 one-sided semantics and describe solutions for providing portable and efficient data sharing, atomic operations, and memory consistency. We describe an implementation of the new interface in the MPICH2 and Open MPI implementations and demonstrate an average performance improvement of 40 % to the communication component of a five-point stencil solver.},
   author = {Torsten Hoefler and James Dinan and Darius Buntinas and Pavan Balaji and Brian Barrett and Ron Brightwell and William Gropp and Vivek Kale and Rajeev Thakur and T Hoefler ETH Zurich and J Dinan and D Buntinas and P Balaji and R Thakur and B Barrett and R Brightwell and W Gropp and V Kale},
   doi = {10.1007/s00607-013-0324-2},
   keywords = {MPI-30 · Shared memory · Hybrid parallel programming Mathematics Subject Classification 68N19 other progamming techniques (objects-oriented,automatic,concurrent,etc),sequential},
   pages = {1121-1136},
   title = {MPI + MPI: a new hybrid approach to parallel programming with MPI plus shared memory},
   volume = {95},
   year = {2013},
}
@report{,
   abstract = {The flux reconstruction (FR) method offers a simple, efficient, and easy to implement method, and it has been shown to equate to a differential approach to discontinuous Galerkin (DG) methods. The FR method is also accurate to an arbitrary order and the isentropic Euler vortex problem is used here to empirically verify this claim. This problem is widely used in computational fluid dynamics (CFD) to verify the accuracy of a given numerical method due to its simplicity and known exact solution at any given time. While verifying our FR solver, multiple obstacles emerged that prevented us from achieving the expected order of accuracy over short and long amounts of simulation time. It was found that these complications stemmed from a few overlooked details in the original problem definition combined with the FR and DG methods achieving high-accuracy with minimal dissipation. This paper is intended to consolidate the many versions of the vortex problem found in literature and to highlight some of the consequences if these overlooked details remain neglected.},
   author = {Seth C Spiegel and H T Huynh and James R Debonis},
   keywords = {CPR,DG,FR,high-order methods},
   title = {A Survey of the Isentropic Euler Vortex Problem using High-Order Methods},
   url = {https://ntrs.nasa.gov/search.jsp?R=20150018403},
}
@report{,
   abstract = {High-performance scientific computing has recently seen a surge of interest in heterogeneous systems, with an emphasis on modern Graphics Processing Units (GPUs). These devices offer tremendous potential for performance and efficiency in important large-scale applications of computational science. However, exploiting this potential can be challenging, as one must adapt to the specialized and rapidly evolving computing environment currently exhibited by GPUs. One way of addressing this challenge is to embrace better techniques and develop tools tailored to their needs. This article presents one simple technique, GPU run-time code generation (RTCG), and PyCUDA, an open-source toolkit that supports this technique. In introducing PyCUDA, this article proposes the combination of a dynamic, high-level scripting language with the massive performance of a GPU as a compelling two-tiered computing platform, potentially offering significant performance and productivity advantages over conventional single-tier, static systems. It is further observed that, compared to competing techniques, the effort required to create codes using run-time code generation with PyCUDA grows more gently in response to growing needs. The concept of RTCG is simple and easily implemented using existing, robust tools. Nonetheless it is powerful enough to support (and encourage) the creation of custom application-specific tools by its users. The premise of the paper is illustrated by a wide range of examples where the technique has been applied with considerable success.},
   author = {Andreas Klöckner and Nicolas Pinto and Yunsup Lee and Bryan Catanzaro and Paul Ivanov and Ahmed Fasih},
   keywords = {Automated Tuning,Code generation,GPU,High-level Languages,Many-core,Massive Parallelism,Single-instruction multiple-data,Software engineering},
   title = {PyCUDA: GPU Run-Time Code Generation for High-Performance Computing},
}
@article{,
   author = {A Klöckner and N Pinto and Y Lee and B Catanzaro and P Ivanov - Parallel Computing and undefined 2012},
   journal = {Elsevier},
   title = {PyCUDA and PyOpenCL: A scripting-based approach to GPU run-time code generation},
   url = {https://www.sciencedirect.com/science/article/pii/S0167819111001281},
}
@article{Alhubail2016,
   abstract = {This article investigates the swept rule of space-time domain decomposition, an idea to break the latency barrier via communicating less often when explicitly solving time-dependent PDEs. The swept rule decomposes space and time among computing nodes in ways that exploit the domains of influence and the domain of dependency, making it possible to communicate once per many timesteps without redundant computation. The article presents simple theoretical analysis to the performance of the swept rule which then was shown to be accurate by conducting numerical experiments.},
   author = {Maitham Alhubail and Qiqi Wang},
   doi = {10.1016/j.jcp.2015.11.026},
   issn = {10902716},
   journal = {Journal of Computational Physics},
   keywords = {Domain decomposition,Latency,Numerical solution of PDE,Parallel computing,Space-time decomposition,Swept rule},
   title = {The swept rule for breaking the latency barrier in time advancing PDEs},
   year = {2016},
}
@article{Magee2018,
   abstract = {The expedient design of precision components in aerospace and other high-tech industries requires simulations of physical phenomena often described by partial differential equations (PDEs) without exact solutions. Modern design problems require simulations with a level of resolution difficult to achieve in reasonable amounts of time—even in effectively parallelized solvers. Though the scale of the problem relative to available computing power is the greatest impediment to accelerating these applications, significant performance gains can be achieved through careful attention to the details of memory communication and access. The swept time–space decomposition rule reduces communication between sub-domains by exhausting the domain of influence before communicating boundary values. Here we present a GPU implementation of the swept rule, which modifies the algorithm for improved performance on this processing architecture by prioritizing use of private (shared) memory, avoiding interblock communication, and overwriting unnecessary values. It shows significant improvement in the execution time of finite-difference solvers for one-dimensional unsteady PDEs, producing speedups of 2–9× for a range of problem sizes, respectively, compared with simple GPU versions and 7–300× compared with parallel CPU versions. However, for a more sophisticated one-dimensional system of equations discretized with a second-order finite-volume scheme, the swept rule performs 1.2–1.9× worse than a standard implementation for all problem sizes.},
   author = {Daniel J Magee and Kyle E Niemeyer},
   doi = {10.1016/j.jcp.2017.12.028},
   issn = {0021-9991},
   journal = {Journal of Computational Physics},
   keywords = {Communication-avoiding algorithms,Computational fluid dynamics,Domain decomposition,GPU computing,High-performance computing,Partial differential equations},
   pages = {338-352},
   title = {Accelerating solutions of one-dimensional unsteady PDEs with GPU-based swept time–space decomposition},
   volume = {357},
   url = {http://www.sciencedirect.com/science/article/pii/S0021999117309221},
   year = {2018},
}
@report{Alhubail2018,
   abstract = {This article describes a method to accelerate parallel, explicit time integration of two-dimensional unsteady PDEs. The method is motivated by our observation that network latency, not bandwidth or computing power, often limits how fast PDEs can be solved in parallel. The method is called the swept rule of space-time domain decomposition. Compared to conventional, space-only domain decomposition, it communicates similar amount of data, but in fewer messages. The swept rule achieves this by decomposing space and time among computing nodes in ways that exploit the domains of influence and the domain of dependency, making it possible to communicate once per many time steps with no redundant computation. By communicating less often, the swept rule effectively breaks the latency barrier, advancing on average more than one time step per ping-pong latency of the network. The article describes the algorithms, presents simple theoretical analysis to the performance of the swept rule in two spatial dimensions, and supports the analysis with numerical experiments.},
   author = {Maitham Alhubail and Qiqi Wang and John Williams},
   keywords = {Domain decomposition,Numerical solution of PDE,Parallel computing * Corresponding author,Space-time decomposition,Swept rule 2D,latency},
   title = {The swept rule for breaking the latency barrier in time advancing two-dimensional PDEs},
   year = {2018},
}
