@inproceedings{Juels2014,
   abstract = {We introduce honey encryption (HE), a simple, general approach to encrypting messages using low min-entropy keys such as passwords. HE is designed to produce a ciphertext which, when decrypted with any of a number of incorrect keys, yields plausible-looking but bogus plaintexts called honey messages. A key benefit of HE is that it provides security in cases where too little entropy is available to withstand brute-force attacks that try every key; in this sense, HE provides security beyond conventional brute-force bounds. HE can also provide a hedge against partial disclosure of high min-entropy keys. HE significantly improves security in a number of practical settings. To showcase this improvement, we build concrete HE schemes for password-based encryption of RSA secret keys and credit card numbers. The key challenges are development of appropriate instances of a new type of randomized message encoding scheme called a distribution-transforming encoder (DTE), and analyses of the expected maximum loading of bins in various kinds of balls-and-bins games. © 2014 International Association for Cryptologic Research.},
   author = {Ari Juels and Thomas Ristenpart},
   doi = {10.1007/978-3-642-55220-5_17},
   isbn = {9783642552199},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {293-310},
   publisher = {Springer Verlag},
   title = {Honey encryption: Security beyond the brute-force bound},
   volume = {8441 LNCS},
   year = {2014},
}
@web_page{,
   title = {Amazon EC2 Pricing - Amazon Web Services},
   url = {https://aws.amazon.com/ec2/pricing/},
}
@article{Hahne2020,
   abstract = {In this paper, we introduce the Python framework PyMGRIT, which implements the multigrid-reduction-in-time (MGRIT) algorithm for solving the (non-)linear systems arising from the discretization of time-dependent problems. The MGRIT algorithm is a reduction-based iterative method that allows parallel-in-time simulations, i. e., calculating multiple time steps simultaneously in a simulation, by using a time-grid hierarchy. The PyMGRIT framework features many different variants of the MGRIT algorithm, ranging from different multigrid cycle types and relaxation schemes, as well as various coarsening strategies, including time-only and space-time coarsening, to using different time integrators on different levels in the multigrid hierachy. PyMGRIT allows serial runs for prototyping and testing of new approaches, as well as parallel runs using the Message Passing Interface (MPI). Here, we describe the implementation of the MGRIT algorithm in PyMGRIT and present the usage from both user and developer point of views. Three examples illustrate different aspects of the package, including pure time parallelism as well as space-time parallelism by coupling PyMGRIT with PETSc or Firedrake, which enable spatial parallelism through MPI.},
   author = {Jens Hahne and Stephanie Friedhoff and Matthias Bolten},
   keywords = {Additional Key Words and Phrases: Multigrid-reduction-in-time (MGRIT), parallel-in-time integration,CCS Concepts: •Mathematics of computing → Solvers,Diierential algebraic equations,Ordinary diierential equations,Partial diierential equations},
   month = {8},
   title = {PyMGRIT: A Python Package for the parallel-in-time method MGRIT},
   url = {http://arxiv.org/abs/2008.05172},
   year = {2020},
}
@web_page{,
   title = {NVIDIA V100 | NVIDIA},
   url = {https://www.nvidia.com/en-us/data-center/v100/},
}
@web_page{,
   title = {GeForce GTX 1080 Ti Graphics Cards | NVIDIA GeForce},
   url = {https://www.nvidia.com/en-sg/geforce/products/10series/geforce-gtx-1080-ti/},
}
@web_page{,
   title = {Intel® Xeon® Processor E5-2698 v4 91753},
   url = {https://www.intel.com/content/www/us/en/products/processors/xeon/e5-processors/e5-2698-v4.html},
}
@web_page{,
   title = {Intel® Xeon® Silver 4114 Processor 123550},
   url = {https://www.intel.com/content/www/us/en/products/processors/xeon/scalable/silver-processors/silver-4114.html},
}
@book_section{Kowarschik2003,
   abstract = {In order to mitigate the impact of the growing gap between CPU speed and main memory performance, today’s computer architectures implement hierarchical memory structures. The idea behind this approach is to hide both the low main memory bandwidth and the latency of main memory accesses which is slow in contrast to the floating-point performance of the CPUs. Usually, there is a small and expensive high speed memory sitting on top of the hierarchy which is usually integrated within the processor chip to provide data with low latency and high bandwidth; i.e., the CPU registers. Moving further away from the CPU, the layers of memory successively become larger and slower. The memory components which are located between the processor core and main memory are called cache memories or caches. They are intended to contain copies of main memory blocks to speed up accesses to frequently needed data [378], [392]. The next lower level of the memory hierarchy is the main memory which is large but also comparatively slow. While external memory such as hard disk drives or remote memory components in a distributed computing environment represent the lower end of any common hierarchical memory design, this paper focuses on optimization techniques for enhancing cache performance.},
   author = {Markus Kowarschik and Christian Weiß},
   doi = {10.1007/3-540-36574-5_10},
   pages = {213-232},
   title = {An Overview of Cache Optimization Techniques and Cache-Aware Numerical Algorithms},
   year = {2003},
}
@report{Lions2013,
   abstract = {Résolution d'EDP par un schéma en temps < pararéel >. Comptes rendus de l'Académie des sciences. Série I, Mathématique, Elsevier, 2001, 332 (7), pp.661-668. ï¿¿hal-00798372ï¿¿},
   author = {Jacques-Louis Lions and Yvon Maday and Gabriel Turinici},
   journal = {Elsevier},
   title = {Résolution d'EDP par un schéma en temps < pararéel >},
   url = {https://hal.archives-ouvertes.fr/hal-00798372},
   year = {2013},
}
@article{Maday2020,
   abstract = {In this paper, we consider the problem of accelerating the numerical simulation of time dependent problems by time domain decomposition. The available algorithms enabling such decompositions present severe efficiency limitations and are an obstacle for the solution of large scale and high dimensional problems. Our main contribution is the improvement of the parallel efficiency of the parareal in time method. The parareal method is based on combining predictions made by a numerically inexpensive solver (with coarse physics and/or coarse resolution) with corrections coming from an expensive solver (with high-fidelity physics and high resolution). At convergence, the algorithm provides a solution that has the fine solver's high-fidelity physics and high resolution. In the classical version, the fine solver has a fixed high accuracy which is the major obstacle to achieve a competitive parallel efficiency. In this paper, we develop an adaptive variant that overcomes this obstacle by dynamically increasing the accuracy of the fine solver across the parareal iterations. We theoretically show that the parallel efficiency becomes very competitive in the ideal case where the cost of the coarse solver is small, thus proving that the only remaining factors impeding full scalability become the cost of the coarse solver and communication time. The developed theory has also the merit of setting a general framework to understand the success of several extensions of parareal based on iteratively improving the quality of the fine solver and re-using information from previous parareal steps. We illustrate the actual performance of the method in stiff ODEs, which are a challenging family of problems since the only mechanism for adaptivity is time and efficiency is affected by the cost of the coarse solver.},
   author = {Y. Maday and O. Mula},
   doi = {10.1016/j.cam.2020.112915},
   issn = {03770427},
   journal = {Journal of Computational and Applied Mathematics},
   keywords = {Convergence rates,Domain decomposition,Inexact fine solver,Parallel efficiency,Parareal in time algorithm,a posteriori estimators},
   month = {10},
   publisher = {Elsevier B.V.},
   title = {An adaptive parareal algorithm},
   volume = {377},
   url = {https://plmlab.math.cnrs.fr/mulahernandez/parareal-adaptive},
   year = {2020},
}
@report{Gander2014,
   abstract = {We present and analyze a new space-time parallel multigrid method for parabolic equations. The method is based on arbitrarily high order discontinuous Galerkin discretizations in time, and a finite element discretization in space. The key ingredient of the new algorithm is a block Jacobi smoother. We present a detailed convergence analysis when the algorithm is applied to the heat equation, and determine asymptotically optimal smoothing parameters, a precise criterion for semi-coarsening in time or full coarsening, and give an asymptotic two grid contraction factor estimate. We then explain how to implement the new multigrid algorithm in parallel, and show with numerical experiments its excellent strong and weak scalability properties. Key words. Space-time parallel methods, multigrid in space-time, DG-discretizations, strong and weak scalability, parabolic problems AMS subject classifications. 65N55, 65F10, 65L60 1. Introduction. About ten years ago, clock speeds of processors have stopped increasing, and the only way to obtain more performance is by using more processing cores. This has led to new generations of supercomputers with millions of computing cores, and even today's small devices are multicore. In order to exploit these new architectures for high performance computing, algorithms must be developed that can use these large numbers of cores efficiently. When solving evolution partial differential equations, the time direction offers itself as a further direction for parallelization, in addition to the spatial directions, and the parareal algorithm [29, 31, 1, 37, 18, 9] has sparked renewed interest in the area of time parallelization, a field that is now just over fifty years old, see the historical overview [8]. We are interested here in space-time parallel methods, which can be based on the two fundamental paradigms of domain decomposition or multigrid. Domain decomposition methods in space-time lead to waveform relaxation type methods, see [17, 7, 19] for classical Schwarz waveform relaxation, [12, 13, 10, 11, 2] for optimal and optimized variants, and [28, 33, 15] for Dirichlet-Neumann and Neumann-Neumann waveform relaxation. The spatial decompositions can be combined with parareal to obtain algorithms that run on arbitrary decompositions of the space-time domain into space-time subdomains, see [32, 14]. Space-time multigrid methods were developed in [20, 30, 41, 25, 40, 26, 27, 43], and reached good F-cycle convergence behavior when appropriate semi-coarsening and extension operators are used. For a variant for non-linear problems, see [4, 36, 35]. We present and analyze here a new space-time parallel multigrid algorithm that has excellent strong and weak scalability properties on large scale parallel computers. As a model problem we consider the heat equation in a bounded domain Ω ⊂ R d , d = 1, 2, 3 with boundary Γ := ∂Ω on the bounded time interval [0, T ],},
   author = {Martin J Gander and Martin Neum¨uller and Neum¨ Neum¨uller},
   journal = {SIAM},
   title = {ANALYSIS OF A NEW SPACE-TIME PARALLEL MULTIGRID ALGORITHM FOR PARABOLIC PROBLEMS},
   url = {https://epubs.siam.org/doi/abs/10.1137/15M1046605},
   year = {2014},
}
@report{,
   abstract = {A novel parallel algorithm for the integration of linear initial-value problems is proposed. This algorithm is based on the simple observation that homogeneous problems can typically be integrated much faster than inhomogeneous problems. An overlapping time-domain decomposition is utilized to obtain decoupled inhomogeneous and homogeneous subproblems, and a near-optimal Krylov method is used for the fast exponential integration of the homogeneous subproblems. We present an error analysis and discuss the parallel scaling of our algorithm. The efficiency of this approach is demonstrated with numerical examples.},
   author = {Martin J Gander and Stefan G ¨ Uttel},
   isbn = {200020131826/1},
   journal = {SIAM},
   keywords = {65F60,65Y05,linear initial-value problem,matrix exponential AMS subject classifications 65L05,parallelization,rational Krylov},
   title = {PARAEXP: A PARALLEL INTEGRATOR FOR LINEAR INITIAL-VALUE PROBLEMS *},
   url = {https://epubs.siam.org/doi/abs/10.1137/110856137},
}
@report{,
   abstract = {The parallel full approximation scheme in space and time (PFASST) introduced by Emmett and Minion in 2012 is an iterative strategy for the temporal parallelization of ODEs and discretized PDEs. As the name suggests, PFASST is similar in spirit to a space-time Full Approximation Scheme (FAS) multigrid method performed over multiple time-steps in parallel. However, since the original focus of PFASST has been on the performance of the method in terms of time par-allelism, the solution of any spatial system arising from the use of implicit or semi-implicit temporal methods within PFASST have simply been assumed to be solved to some desired accuracy completely at each sub-step and each iteration by some unspecified procedure. It hence is natural to investigate how iterative solvers in the spatial dimensions can be interwoven with the PFASST iterations and whether this strategy leads to a more efficient overall approach. This paper presents an initial investigation on the relative performance of different strategies for coupling PFASST iterations with multigrid methods for the implicit treatment of diffusion terms in PDEs. In particular, we compare full accuracy multigrid solves at each sub-step with a small fixed number of multigrid V-cycles. This reduces the cost of each PFASST iteration at the possible expense of a corresponding increase in the number of PFASST iterations needed for convergence. Parallel efficiency of the resulting methods is explored through numerical examples. Key words. Parallel in time, PFASST, multigrid 1. Introduction. The past decade has seen a growing interest in the development of parallel methods for temporal integration of ordinary differential equations (ODEs), particularly in the context of temporal strategies for partial differential equations (PDEs). One factor fueling this interest is related to the evolution of supercom-puters during this time. Since the end of the exponential increase in individual processors speeds, increases in supercomputer speeds have been mostly due to increases in the number of computational cores, and current projections suggest that the first exaflop computer will contain on the order of a billion cores [14]. The implication of this trend is that increasing concurrency in algorithms is essential, and in the case of time-dependent PDE simulations, the use of space-time parallelism is an attractive option. Time-parallel methods have a long history dating back at least to the work of Nievergelt [26]. In the context of space-time multigrid, Hackbusch noted already in 1984 that relaxation operators in parabolic multigrid can be employed on multiple time steps simultaneously [15]. The 1997 review article by Burrage [7] provides a summary of early work on the subject. More recently, the parareal method proposed in 2001 [22] has renewed interest in temporal parallelization methods. In 2012 the parallel full approximation scheme in space and time (PFASST) was introduced by Emmett and Minion [9], and performance results for PFASST using space-time par-allelization with hundreds of thousands of cores can be found in [31, 28]. The PFASST algorithm is based on a type of deferred corrections strategy for ODEs [8], with corrections being applied on multiple time steps in parallel. As such, there are similarities between parareal and PFASST (see [25, 24]). On the other hand, the parallel efficiency of PFASST depends on the construction of a hierarchy of space-time discretizations, hence there are also similarities between PFASST and space},
   author = {M L Minion and R Speck and M Bolten and M Emmett and D Ruprecht},
   journal = {SIAM},
   title = {INTERWEAVING PFASST AND PARALLEL MULTIGRID},
   url = {https://epubs.siam.org/doi/abs/10.1137/14097536X},
}
@article{,
   author = {M Emmett and M Minion - in Applied Mathematics and Computational Science and undefined 2012},
   journal = {msp.org},
   title = {Toward an efficient parallel in time method for partial differential equations},
   url = {https://msp.org/camcos/2012/7-1/p04.xhtml},
}
@article{Wu2018,
   abstract = {It is challenge work to design parareal algorithms for time-fractional differential equations due to the historical effect of the fractional operator. A direct extension of the classical parareal method to such equations will lead to unbalance computational time in each process. In this work, we present an efficient parareal iteration scheme to overcome this issue, by adopting two recently developed local time-integrators for time fractional operators. In both approaches, one introduces auxiliary variables to localized the fractional operator. To this end, we propose a new strategy to perform the coarse grid correction so that the auxiliary variables and the solution variable are corrected separately in a mixed pattern. It is shown that the proposed parareal algorithm admits robust rate of convergence. Numerical examples are presented to support our conclusions.},
   author = {Shu-Lin Wu and Tao Zhou},
   doi = {10.1016/j.jcp.2017.12.029},
   journal = {Article in Journal of Computational Physics},
   keywords = {Local time-integrators,Parareal,Time-fractional differential equations},
   pages = {135-149},
   title = {Parareal algorithms with local time-integrators for time fractional differential equations Fast Computation Algorithms for Differential Equations View project Numerical methods for time and space fractional partial differential equations View project Parareal algorithms with local time-integrators for time fractional differential equations ✩},
   volume = {358},
   url = {www.elsevier.com/locate/jcp},
   year = {2018},
}
@article{Falgout2017,
   abstract = {The need for parallelism in the time dimension is being driven by changes in computer architectures, where performance increases are now provided through greater concurrency, not faster clock speeds. This creates a bottleneck for sequential time marching schemes because they lack parallelism in the time dimension. Multigrid reduction in time (MGRIT) is an iterative procedure that allows for temporal parallelism by utilizing multigrid reduction techniques and a multilevel hierarchy of coarse time grids. MGRIT has been shown to be effective for linear problems, with speedups of up to 50 times. The goal of this work is the eficient solution of nonlinear problems with MGRIT, where eficiency is defined as achieving similar performance when compared to an equivalent linear problem. The benchmark nonlinear problem is the p-Laplacian, where p = 4 corresponds to a well-known nonlinear diffusion equation and p = 2 corresponds to the standard linear diffusion operator, our benchmark linear problem. The key dificulty encountered is that the nonlinear timestep solver becomes progressively more expensive on coarser time levels as the time-step size increases. To overcome such dificulties, multigrid research has historically targeted an accumulated body of experience regarding how to choose an appropriate solver for a specific problem type. To that end, this paper develops a library of MGRIT optimizations and modifications, most important an alternate initial guess for the nonlinear time-step solver and delayed spatial coarsening, that will allow many nonlinear parabolic problems to be solved with parallel scaling behavior comparable to the corresponding linear problem.},
   author = {R. D. Falgout and T. A. Manteuffel and B. O'Neill and J. B. Schroder},
   doi = {10.1137/16M1082330},
   issn = {10957197},
   issue = {5},
   journal = {SIAM Journal on Scientific Computing},
   keywords = {High performance computing,Multigrid,Multigrid-in-time,Nonlinear,Parabolic problems,Parareal,Reduction-based multigrid},
   pages = {S298-S322},
   publisher = {Society for Industrial and Applied Mathematics Publications},
   title = {Multigrid reduction in time for nonlinear parabolic problems: A case study},
   volume = {39},
   year = {2017},
}
@article{Falgout2014,
   abstract = {We consider optimal-scaling multigrid solvers for the linear systems that arise from the discretization of problems with evolutionary behavior. Typically, solution algorithms for evolution equations are based on a time-marching approach, solving sequentially for one time step after the other. Parallelism in these traditional time-integration techniques is limited to spatial parallelism. However, current trends in computer architectures are leading toward systems with more, but not faster, processors. Therefore, faster compute speeds must come from greater parallelism. One approach to achieving parallelism in time is with multigrid, but extending classical multigrid methods for elliptic operators to this setting is not straightforward. In this paper, we present a nonintrusive, optimal-scaling time-parallel method based on multigrid reduction (MGR). We demonstrate optimality of our multigrid-reduction-in-time algorithm (MGRIT) for solving diffusion equations in two and three space dimensions in numerical experiments. Furthermore, through both parallel performance models and actual parallel numerical results, we show that we can achieve significant speedup in comparison to sequential time marching on modern architectures.},
   author = {R. D. Falgout and S. Friedhoff and Tz V. Kolev and S. P. MacLachlan and J. B. Schroder},
   doi = {10.1137/130944230},
   issn = {10957200},
   issue = {6},
   journal = {SIAM Journal on Scientific Computing},
   keywords = {Multigrid-in-time,Parabolic problems,Parareal,Reduction-based multigrid},
   pages = {C635-C661},
   publisher = {Society for Industrial and Applied Mathematics Publications},
   title = {Parallel time integration with multigrid},
   volume = {36},
   year = {2014},
}
@report{,
   abstract = {Time parallel time integration methods have received renewed interest over the last decade because of the advent of massively parallel computers, which is mainly due to the clock speed limit reached on today's processors. When solving time dependent partial differential equations, the time direction is usually not used for parallelization. But when parallelization in space saturates, the time direction offers itself as a further direction for parallelization. The time direction is however special, and for evolution problems there is a causality principle: the solution later in time is affected (it is even determined) by the solution earlier in time, but not the other way round. Algorithms trying to use the time direction for parallelization must therefore be special, and take this very different property of the time dimension into account. We show in this chapter how time domain decomposition methods were invented, and give an overview of the existing techniques. Time parallel methods can be classified into four different groups: methods based on multiple shooting, methods based on domain decomposition and waveform relaxation, space-time multigrid methods and direct time parallel methods. We show for each of these techniques the main inventions over time by choosing specific publications and explaining the core ideas of the authors. This chapter is for people who want to quickly gain an overview of the exciting and rapidly developing area of research of time parallel methods.},
   author = {Martin J Gander},
   journal = {Springer},
   title = {50 Years of Time Parallel Time Integration},
   url = {https://link.springer.com/chapter/10.1007/978-3-319-23321-5_3},
}
@report{,
   abstract = {Many large-scale scientific computations require eigenvalue solvers in a scaling regime where efficiency is limited by data movement. We introduce a parallel algorithm for computing the eigenvalues of a dense symmetric matrix, which performs asymptotically less communication than previously known approaches. We provide analysis in the Bulk Synchronous Parallel (BSP) model with additional consideration for communication between a local memory and cache. Given sufficient memory to store c copies of the symmetric matrix, our algorithm requires Θ(√ c) less interprocessor communication than previously known algorithms, for any c ≤ p 1/3 when using p processors. The algorithm first reduces the dense symmetric matrix to a banded matrix with the same eigenvalues. Subsequently, the algorithm employs successive reduction to O(log p) thinner banded matrices. We employ two new parallel algorithms that achieve lower communication costs for the full-to-band and band-to-band reductions. Both of these algorithms leverage a novel QR factorization algorithm for rectangular matrices.},
   author = {Edgar Solomonik and Eth Zurich and Grey Ballard and James Demmel},
   journal = {dl.acm.org},
   title = {A communication-avoiding parallel algorithm for the symmetric eigenvalue problem Torsten Hoefler},
   url = {https://dl.acm.org/doi/abs/10.1145/3087556.3087561},
}
@report{Khabou2012,
   author = {Amal Khabou and James W Demmel and Ming Gu},
   journal = {SIAM},
   title = {LU factorization with panel rank revealing pivoting and its communication avoiding version},
   url = {https://epubs.siam.org/doi/abs/10.1137/120863691},
   year = {2012},
}
@article{Ballard2011,
   abstract = {In 1981 Hong and Kung proved a lower bound on the amount of communication (amount of data moved between a small, fast memory and large, slow memory) needed to perform dense, n-by-n matrix multiplication using the conventional O(n3) algorithm, where the input matrices were too large to fit in the small, fast memory. In 2004 Irony, Toledo, and Tiskin gave a new proof of this result and extended it to the parallel case (where communication means the amount of data moved between processors). In both cases the lower bound may be expressed as ω(arithmetic operations/pM), where M is the size of the fast memory (or local memory in the parallel case). Here we generalize these results to a much wider variety of algorithms, including LU factorization, Cholesky factorization, LDLT factorization, QR factorization, the Gram-Schmidt algorithm, and algorithms for eigenvalues and singular values, i.e., essentially all direct methods of linear algebra. The proof works for dense or sparse matrices and for sequential or parallel algorithms. In addition to lower bounds on the amount of data moved (bandwidth cost), we get lower bounds on the number of messages required to move it (latency cost). We extend our lower bound technique to compositions of linear algebra operations (like computing powers of a matrix) to decide whether it is enough to call a sequence of simpler optimal algorithms (like matrix multiplication) to minimize communication, or whether we can do better. We give examples of both. We also show how to extend our lower bounds to certain graph-theoretic problems. We point out recently designed algorithms that attain many of these lower bounds. © 2011 Society for Industrial and Applied Mathematics.},
   author = {Grey Ballard and James Demmel and Olga Holtz and Oded Schwartz},
   doi = {10.1137/090769156},
   issn = {08954798},
   issue = {3},
   journal = {SIAM Journal on Matrix Analysis and Applications},
   keywords = {Bandwidth,Communication-avoiding,Latency,Linear algebra algorithms,Lower bound},
   pages = {866-901},
   title = {Minimizing communication in numerical linear algebra},
   volume = {32},
   year = {2011},
}
@book{,
   abstract = {The performance of sparse iterative solvers is typically limited by sparse matrix-vector multiplication, which is itself limited by memory system and network performance. As the gap between computation and communication speed continues to widen, these traditional sparse methods will suffer. In this paper we focus on an alternative building block for sparse iterative solvers, the "matrix powers ker-nel" [x, Ax, A 2 x,. .. , A k x], and show that by organizing computations around this kernel, we can achieve near-minimal communication costs. We consider communication very broadly as both network communication in parallel code and memory hierarchy access in sequential code. In particular, we introduce a parallel algorithm for which the number of messages (total latency cost) is independent of the power k, and a sequential algorithm, that reduces both the number and volume of accesses, so that it is independent of k in both latency and bandwidth costs. This is part of a larger project to develop "communication-avoiding Krylov subspace methods," which also addresses the numerical issues associated with these methods. Our algorithms work for general sparse matrices that "partition well". We introduce parallel performance models of matrices arising from 2D and 3D problems and show predicted speedups over a conventional algorithm of up to 7x on a Petaflop-scale machine and up to 22x on computation across the Grid. Analogous sequential performance models of the same problems predict speedups over a conventional algorithm of up to 10x on an out-of-core implementation , and up to 2.5x when we use our ideas to reduce off-chip latency and bandwidth to DRAM. Finally, we validate the model on an out-of-core sequential implementation and measured a speedup of over 3x, which is close to the predicted speedup.},
   author = {James Demmel and Mark Hoemmen and Marghoob Mohiyuddin and Katherine Yelick},
   isbn = {9781424416943},
   journal = {ieeexplore.ieee.org},
   title = {Avoiding Communication in Sparse Matrix Computations},
   url = {https://ieeexplore.ieee.org/abstract/document/4536305/},
}
@article{,
   author = {M Baboulin and S Donfack and J Dongarra and L Grigori - Procedia Computer … and undefined 2012},
   journal = {Elsevier},
   title = {A class of communication-avoiding algorithms for solving general dense linear systems on CPU/GPU parallel machines},
   url = {https://www.sciencedirect.com/science/article/pii/S187705091200124X},
}
@book_section{Gander2015,
   abstract = {Time parallel time integration methods have received renewed interest over the last decade because of the advent of massively parallel computers, which is mainly due to the clock speed limit reached on today's processors. When solving time dependent partial differential equations, the time direction is usually not used for parallelization. But when parallelization in space saturates, the time direction offers itself as a further direction for parallelization. The time direction is however special, and for evolution problems there is a causality principle: the solution later in time is affected (it is even determined) by the solution earlier in time, but not the other way round. Algorithms trying to use the time direction for parallelization must therefore be special, and take this very different property of the time dimension into account. We show in this chapter how time domain decomposition methods were invented, and give an overview of the existing techniques. Time parallel methods can be clas-sified into four different groups: methods based on multiple shooting, methods based on domain decomposition and waveform relaxation, space-time multigrid methods and direct time parallel methods. We show for each of these techniques the main inventions over time by choosing specific publications and explaining the core ideas of the authors. This chapter is for people who want to quickly gain an overview of the exciting and rapidly developing area of research of time parallel methods.},
   author = {Martin J. Gander},
   doi = {10.1007/978-3-319-23321-5_3},
   pages = {69-113},
   title = {50 Years of Time Parallel Time Integration},
   year = {2015},
}
@article{Ghorpade2012,
   abstract = {The future of computation is the Graphical Processing Unit, i.e. the GPU. The promise that the graphics cards have shown in the field of image processing and accelerated rendering of 3D scenes, and the computational capability that these GPUs possess, they are developing into great parallel computing units. It is quite simple to program a graphics processor to perform general parallel tasks. But after understanding the various architectural aspects of the graphics processor, it can be used to perform other taxing tasks as well. In this paper, we will show how CUDA can fully utilize the tremendous power of these GPUs. CUDA is NVIDIA's parallel computing architecture. It enables dramatic increases in computing performance, by harnessing the power of the GPU. This paper talks about CUDA and its architecture. It takes us through a comparison of CUDA C/C++ with other parallel programming languages like OpenCL and DirectCompute. The paper also lists out the common myths about CUDA and how the future seems to be promising for CUDA.},
   author = {Jayshree Ghorpade and Jitendra Parande and Madhura Kulkarni and Amit Bawaskar},
   doi = {10.5121/acij.2012.3109},
   issue = {1},
   journal = {Advanced Computing: An International Journal ( ACIJ )},
   keywords = {ALU,CUDA,DirectCompute,GFLOPS,GPGPU,GPU,OpenCL,block,data parallelism,grid,thread},
   title = {GPGPU PROCESSING IN CUDA ARCHITECTURE},
   volume = {3},
   year = {2012},
}
@generic{Alexandrov2016,
   abstract = {This editorial outlines the research context, the needs and challenges on the route to exascale. In particular the focus is on novel mathematical methods and mathematical modeling approaches together with scalable scientific algorithms that are needed to enable key science applications at extreme-scale. This is especially true as HPC systems continue to scale up in compute node and processor core count. These extreme-scale systems require novel mathematical methods to be developed that lead to scalable scientific algorithms to hide network and memory latency, have very high computation/communication overlap, have minimal communication, have fewer synchronization points. It stresses the need of scalability at all levels, starting from mathematical methods level through algorithmic level, and down to systems level in order to achieve overall scalability. It also points out that with the advances of Data Science in the past few years the need of such scalable mathematical methods and algorithms able to handle data and compute intensive applications at scale becomes even more important. The papers in the special issue are selected to address one or several key challenges on the route to exascale.},
   author = {Vassil Alexandrov},
   doi = {10.1016/j.jocs.2016.04.014},
   issn = {18777503},
   journal = {Journal of Computational Science},
   keywords = {Computational Science research methods,Exascale computing,HPC,Novel mathematical methods,Scalable algorithms},
   month = {5},
   pages = {1-4},
   publisher = {Elsevier B.V.},
   title = {Route to exascale: Novel mathematical methods, scalable algorithms and Computational Science skills},
   volume = {14},
   url = {http://dx.doi.org/10.1016/j.jocs.2016.04.014},
   year = {2016},
}
@article{Collette2008,
   author = {A Collette},
   title = {HDF5 for Python},
   year = {2008},
}
@report{Leveque2002,
   author = {Randall J Leveque},
   isbn = {0521810876},
   title = {Finite Volume Methods for Hyperbolic Problems},
   url = {http://www.cambridge.org},
   year = {2002},
}
@article{Magee2020,
   abstract = {Applications that exploit the architectural details of high-performance computing (HPC) systems have become increasingly invaluable in academia and industry over the past two decades. The most important hardware development of the last decade in HPC has been the general purpose graphics processing unit (GPGPU), a class of massively parallel devices that now contributes the majority of computational power in the top 500 supercomputers. As these systems grow, small costs such as latency—due to the fixed cost of memory accesses and communication—accumulate in a large simulation and become a significant barrier to performance. The swept time-space decomposition rule is a communication-avoiding technique for time-stepping stencil update formulas that attempts to reduce latency costs. This work extends the swept rule by targeting heterogeneous, CPU/GPU architectures representing current and future HPC systems. We compare our approach to a naive decomposition scheme with two test equations using an MPI+CUDA pattern on 40 processes over two nodes containing one GPU. The swept rule produces a factor of 1.9 to 23 speedup for the heat equation and a factor of 1.1 to 2.0 speedup for the Euler equations, using the same processors and work distribution, and with the best possible configurations. These results show the potential effectiveness of the swept rule for different equations and numerical schemes on massively parallel compute systems that incur substantial latency costs.},
   author = {Daniel J. Magee and Anthony S. Walker and Kyle E. Niemeyer},
   doi = {10.1007/s11227-020-03340-9},
   issn = {15730484},
   journal = {Journal of Supercomputing},
   keywords = {Communication-avoiding algorithms,Computational fluid dynamics,Domain decomposition,Heterogeneous computing,Partial differential equations},
   publisher = {Springer},
   title = {Applying the swept rule for solving explicit partial differential equations on heterogeneous computing systems},
   year = {2020},
}
@report{,
   abstract = {MPI for Python provides bindings of the Message Passing Interface (MPI) standard for the Python programming language and allows any Python program to exploit multiple processors. This package is constructed on top of the MPI-1 specification and defines an object oriented interface which closely follows MPI-2 C++ bindings. It supports point-to-point (sends, receives) and collective (broadcasts, scatters, gathers) communications of general Python objects. Efficiency has been tested in a Beowulf class cluster and satisfying results were obtained. MPI for Python is open source and available for download on the web (},
   author = {Lisandro Dalcín and Rodrigo Paz and Mario Storti},
   journal = {Elsevier},
   keywords = {High level languages,MPI,Message passing,Parallel Python},
   title = {MPI for Python},
   url = {http://www.cimec.org.ar/python},
}
@article{Walker2020,
   author = {Anthony Walker},
   doi = {10.5281/ZENODO.3891934},
   keywords = {cantera,exhaust,python},
   month = {6},
   title = {pyplume},
   url = {https://doi.org/10.5281/zenodo.3891934#.XuPwuNU5_pY.mendeley},
   year = {2020},
}
@article{Christian2018,
   abstract = {Making sense of modeled atmospheric composition requires not only comparison to in situ measurements but also knowing and quantifying the sensitivity of the model to its input factors. Using a global sensitivity method involving the simultaneous perturbation of many chemical transport model input factors, we find the model uncertainty for ozone (O 3), hydroxyl radical (OH), and hydroperoxyl radical (HO 2) mixing ratios, and apportion this uncertainty to specific model inputs for the DC-8 flight tracks corresponding to the NASA Intercontinental Chemical Transport Experiment (INTEX) campaigns of 2004 and 2006. In general, when uncertainties in modeled and measured quantities are accounted for, we find agreement between modeled and measured ox-idant mixing ratios with the exception of ozone during the Houston flights of the INTEX-B campaign and HO 2 for the flights over the northernmost Pacific Ocean during INTEX-B. For ozone and OH, modeled mixing ratios were most sensitive to a bevy of emissions, notably lightning NO x , various surface NO x sources, and isoprene. HO 2 mixing ratios were most sensitive to CO and isoprene emissions as well as the aerosol uptake of HO 2. With ozone and OH being generally overpredicted by the model, we find better agreement between modeled and measured vertical profiles when reducing NO x emissions from surface as well as lightning sources.},
   author = {Kenneth E Christian and William H Brune and Jingqiu Mao and Xinrong Ren},
   doi = {10.5194/acp-18-2443-2018},
   journal = {Atmos. Chem. Phys},
   pages = {2443-2460},
   title = {Global sensitivity analysis of GEOS-Chem modeled ozone and hydrogen oxides during the INTEX campaigns},
   volume = {18},
   url = {https://doi.org/10.5194/acp-18-2443-2018},
   year = {2018},
}
@article{Kelp2018,
   abstract = {Chemical transport models (CTMs), which simulate air pollution transport, transformation, and removal, are computationally expensive, largely because of the computational intensity of the chemical mechanisms: systems of coupled differential equations representing atmospheric chemistry. Here we investigate the potential for machine learning to reproduce the behavior of a chemical mechanism, yet with reduced computational expense. We create a 17-layer residual multi-target regression neural network to emulate the Carbon Bond Mechanism Z (CBM-Z) gas-phase chemical mechanism. We train the network to match CBM-Z predictions of changes in concentrations of 77 chemical species after one hour, given a range of chemical and meteorological input conditions, which it is able to do with root-mean-square error (RMSE) of less than 1.97 ppb (median RMSE = 0.02 ppb), while achieving a 250x computational speedup. An additional 17x speedup (total 4250x speedup) is achieved by running the neural network on a graphics-processing unit (GPU). The neural network is able to reproduce the emergent behavior of the chemical system over diurnal cycles using Euler integration, but additional work is needed to constrain the propagation of errors as simulation time progresses.},
   author = {Makoto M. Kelp and Christopher W. Tessum and Julian D. Marshall},
   month = {8},
   title = {Orders-of-magnitude speedup in atmospheric chemistry modeling through neural network-based emulation},
   url = {http://arxiv.org/abs/1808.03874},
   year = {2018},
}
@book{Stocker2013,
   abstract = {This latest Fifth Assessment Report of the Intergovernmental Panel on Climate Change (IPCC) will again form the standard scientific reference for all those concerned with climate change and its consequences, including students and researchers in environmental science, meteorology, climatology, biology, ecology and atmospheric chemistry. It provides invaluable material for decision makers and stakeholders: international, national, local; and in all branches: government, businesses, and NGOs. This volume provides: An authoritative and unbiased overview of the physical science basis of climate change • A more extensive assessment of changes observed throughout the climate system than ever before • New dedicated chapters on sea-level change, biogeochemical cycles, clouds and aerosols, and regional climate phenomena • A more extensive coverage of model projections, both near-term and long-term climate projections • A detailed assessment of climate change observations, modelling, and attribution for every continent • A new comprehensive atlas of global and regional climate projections for 35 regions of the world.},
   author = {Thomas F. Stocker and Dahe Qin and Gian Kasper Plattner and Melinda M.B. Tignor and Simon K. Allen and Judith Boschung and Alexander Nauels and Yu Xia and Vincent Bex and Pauline M. Midgley},
   doi = {10.1017/CBO9781107415324},
   isbn = {9781107415324},
   journal = {Climate Change 2013 the Physical Science Basis: Working Group I Contribution to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change},
   month = {1},
   pages = {1-1535},
   publisher = {Cambridge University Press},
   title = {Climate change 2013 the physical science basis: Working Group I contribution to the fifth assessment report of the intergovernmental panel on climate change},
   volume = {9781107057999},
   year = {2013},
}
@article{Burkhardt2011,
   abstract = {Aviation makes a significant contribution to anthropogenic climate forcing. The impacts arise from emissions of greenhouse gases, aerosols and nitrogen oxides, and from changes in cloudiness in the upper troposphere. An important but poorly understood component of this forcing is caused by 'contrail cirrus'-a type of cloud that consist of young line-shaped contrails and the older irregularly shaped contrails that arise from them. Here we use a global climate model that captures the whole life cycle of these man-made clouds to simulate their global coverage, as well as the changes in natural cloudiness that they induce. We show that the radiative forcing associated with contrail cirrus as a whole is about nine times larger than that from line-shaped contrails alone. We also find that contrail cirrus cause a significant decrease in natural cloudiness, which partly offsets their warming effect. Nevertheless, net radiative forcing due to contrail cirrus remains the largest single radiative-forcing component associated with aviation. Our findings regarding global radiative forcing by contrail cirrus will allow their effects to be included in studies assessing the impacts of aviation on climate and appropriate mitigation options.},
   author = {Ulrike Burkhardt and Bernd Kärcher},
   doi = {10.1038/NCLIMATE1068},
   journal = {nature.com},
   title = {Global radiative forcing from contrail cirrus},
   url = {www.nature.com/natureclimatechange},
   year = {2011},
}
@generic{Airbus2019,
   author = {Global Market Forecast Airbus},
   publisher = {France},
   title = {Global Networks Global Citizens, 2019-2038},
   year = {2019},
}
@article{Caiazzo2017,
   abstract = {Contrails and contrail-cirrus may be the largest source of radiative forcing (RF) attributable to aviation. Biomass-derived alternative jet fuels are a potentially major way to mitigate the climate impacts of aviation by reducing lifecycle CO2 emissions. Given the up to 90% reduction in soot emissions from paraffinic biofuels, the potential for a significant impact on contrail RF due to the reduction in contrail-forming ice nuclei (IN) remains an open question. We simulate contrail formation and evolution to quantify RF over the United States under different emissions scenarios. Replacing conventional jet fuels with paraffinic biofuels generates two competing effects. First, the higher water emissions index results in an increase in contrail occurrence (∼ +8%). On the other hand, these contrails are composed of larger diameter crystals (∼ +58%) at lower number concentrations (∼ -75%), reducing both contrail optical depth (∼ -29%) and albedo (∼ -32%). The net changes in contrail RF induced by switching to biofuels range from -4% to +18% among a range of assumed ice crystal habits (shapes). In comparison, cleaner burning engines (with no increase in water emissions index) result in changes to net contrail RF ranging between -13% and +5% depending on habit. Thus, we find that even 67% to 75% reductions in aircraft soot emissions are insufficient to substantially reduce warming from contrails, and that the use of biofuels may either increase or decrease contrail warming - contrary to previous expectations of a significant decrease in warming.},
   author = {Fabio Caiazzo and Akshat Agarwal and Raymond L. Speth and Steven R.H. Barrett},
   doi = {10.1088/1748-9326/aa893b},
   issn = {17489326},
   issue = {11},
   journal = {Environmental Research Letters},
   keywords = {alternative fuels,aviation,biofuels,climate,contrail cirrus,contrails},
   month = {11},
   publisher = {Institute of Physics Publishing},
   title = {Impact of biofuels on contrail warming},
   volume = {12},
   year = {2017},
}
@article{Fritz2020,
   abstract = {Abstract. Emissions from aircraft engines contribute to atmospheric NOx, driving changes in both the climate and in surface air quality. Existing atmospheric models typically assume instant dilution of emissions into large-scale grid cells, neglecting non-linear, small-scale processes occurring in aircraft wakes. They also do not explicitly simulate the formation of ice crystals, which could drive local chemical processing. This assumption may lead to errors in estimates of aircraft-attributable ozone production, and in turn to biased estimates of aviation's current impacts on the atmosphere and the effect of future changes in emissions. This includes black carbon emissions, on which contrail ice forms. These emissions are expected to reduce as biofuel usage increases, but their chemical effects are not well captured by existing models. To address this problem, we develop a Lagrangian model that explicitly models the chemical and microphysical evolution of an aircraft plume. It includes a unified tropospheric–stratospheric chemical mechanism that incorporates heterogeneous chemistry on background and aircraft-induced aerosols. Microphysical processes are also simulated, including the formation, persistence, and chemical influence of contrails. The plume model is used to quantify how the long-term (24 h) atmospheric chemical response to an aircraft plume varies in response to different environmental conditions, engine characteristics, and fuel properties. We find that an instant-dilution model consistently overestimates ozone production compared to the plume model, up to a maximum error of ∼200 % at cruise altitudes. Instant dilution of emissions also underestimates the fraction of remaining NOx, although the magnitude and sign of the error vary with season, altitude, and latitude. We also quantify how changes in black carbon emissions affect plume behavior. Our results suggest that a 50 % reduction in black carbon emissions, as may be possible through blending with certain biofuels, may lead to thinner, shorter-lived contrails. For the cases that we modeled, these contrails sublimate ∼5 % to 15 % sooner and are 10 % to 22 % optically thinner. The conversion of emitted NOx to HNO3 and N2O5 falls by 16 % and 33 %, respectively, resulting in chemical feedbacks that are not resolved by instant-dilution approaches. The persistent discrepancies between results from the instant-dilution approach and from the aircraft plume model demonstrate that a parameterization of effective emission indices should be incorporated into 3-D atmospheric chemistry transport models.},
   author = {Thibaud M. Fritz and Sebastian D. Eastham and Raymond L. Speth and Steven R. H. Barrett},
   doi = {10.5194/acp-20-5697-2020},
   issn = {16807324},
   issue = {9},
   journal = {Atmospheric Chemistry and Physics},
   month = {5},
   pages = {5697-5727},
   publisher = {Copernicus GmbH},
   title = {The role of plume-scale processes in long-term impacts of aircraft emissions},
   volume = {20},
   year = {2020},
}
@article{Gao2016,
   abstract = {Reaction Mechanism Generator (RMG) constructs kinetic models composed of elementary chemical reaction steps using a general understanding of how molecules react. Species thermochemistry is estimated through Benson group additivity and reaction rate coefficients are estimated using a database of known rate rules and reaction templates. At its core, RMG relies on two fundamental data structures: graphs and trees. Graphs are used to represent chemical structures, and trees are used to represent thermodynamic and kinetic data. Models are generated using a rate-based algorithm which excludes species from the model based on reaction fluxes. RMG can generate reaction mechanisms for species involving carbon, hydrogen, oxygen, sulfur, and nitrogen. It also has capabilities for estimating transport and solvation properties, and it automatically computes pressure-dependent rate coefficients and identifies chemically-activated reaction paths. RMG is an object-oriented program written in Python, which provides a stable, robust programming architecture for developing an extensible and modular code base with a large suite of unit tests. Computationally intensive functions are cythonized for speed improvements. Program summary Program title: RMG Catalogue identifier: AEZW-v1-0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEZW-v1-0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: MIT/X11 License No. of lines in distributed program, including test data, etc.: 958681 No. of bytes in distributed program, including test data, etc.: 9495441 Distribution format: tar.gz Programming language: Python. Computer: Windows, Ubuntu, and Mac OS computers with relevant compilers. Operating system: Unix/Linux/Windows. RAM: 1 GB minimum, 16 GB or more for larger simulations Classification: 16.12. External routines: RDKit, Open Babel, DASSL, DASPK, DQED, NumPy, SciPy Nature of problem: Automatic generation of chemical kinetic mechanisms for molecules containing C, H, O, S, and N. Solution method: Rate-based algorithm adds most important species and reactions to a model, with rate constants derived from rate rules and other parameters estimated via group additivity methods. Additional comments: The RMG software package also includes CanTherm, a tool for computing the thermodynamic properties of chemical species and both high-pressure-limit and pressure-dependent rate coefficients for chemical reactions using results from quantum chemical calculations. CanTherm is compatible with a variety of ab initio quantum chemistry software programs, including but not limited to Gaussian, MOPAC, QChem, and MOLPRO. Running time: From 30 s for the simplest molecules, to up to several weeks, depending on the size of the molecule and the conditions of the reaction system chosen.},
   author = {Connie W. Gao and Joshua W. Allen and William H. Green and Richard H. West},
   doi = {10.1016/j.cpc.2016.02.013},
   issn = {00104655},
   journal = {Computer Physics Communications},
   keywords = {Automatic reaction mechanism generation,Chemical kinetics,Combustion,Rate-based algorithm},
   month = {6},
   pages = {212-225},
   publisher = {Elsevier B.V.},
   title = {Reaction Mechanism Generator: Automatic construction of chemical kinetic mechanisms},
   volume = {203},
   year = {2016},
}
@article{Turns1996,
   author = {SR Turns},
   title = {Introduction to combustion},
   url = {http://jmargolin.com/flame/refs/ref12_turns.pdf},
   year = {1996},
}
@article{,
   author = {NJ Curtis and KE Niemeyer and CJ Sung - Combustion and Flame and undefined 2018},
   journal = {Elsevier},
   title = {Using SIMD and SIMT vectorization to evaluate sparse chemical kinetic Jacobian matrices and thermochemical source terms},
   url = {https://www.sciencedirect.com/science/article/pii/S0010218018303997},
}
@report{Livermore2003,
   abstract = {CVODES, which is part of the SUNDIALS software suite, is a stiff and nonstiff ordinary differential equation initial value problem solver with sensitivity analysis capabilities. CVODES is written in a data-independent manner, with a highly modular structure to allow incorporation of different preconditioning and/or linear solver methods. It shares with the other SUNDIALS solvers several common modules, most notably the generic kernel of vector operations and a set of generic linear solvers and preconditioners. CVODES solves the IVP by one of two methods-backward differentiation formula or Adams-Moulton-both implemented in a variable-step, variable-order form. The forward sensitivity module in CVODES implements the simultaneous corrector method, as well as two flavors of staggered corrector methods. Its adjoint sensitivity module provides a combination of checkpoint-ing and cubic Hermite interpolation for the efficient generation of the forward solution during the adjoint system integration. We describe the current capabilities of CVODES, its design principles, and connection to the SUNDIALS suite, and the user interface. Finally, we mention current and future development efforts for CVODES, particularly in the direction of automatic generation of the sensitivity right-hand sides using automatic differentiation and/or complex-step techniques.},
   author = {Lawrence Livermore and Radu Serban and Alan C Hindmarsh},
   keywords = {G17 [Numerical Analysis]: Ordinary Differential Equations-Initial value problems,G4 [Mathematical Software]: Algorithm design and anal-ysis,Mul-tistep methods,Stiff equations General Terms: Algorithms, Design Additional Key Words and Phrases: ODEs, Forward Sensitivity Analysis, Adjoint Sensitivity Analysis},
   title = {Approved for public release; further dissemination unlimited CVODES: An ODE Solver with Sensitivity Analysis Capabilities CVODES: An ODE solver with sensitivity analysis capabilities},
   url = {https://computing.llnl.gov/casc/nsde/pubs/toms_cvodes_with_covers.pdf},
   year = {2003},
}
@article{Cohen1996,
   abstract = {CVODE is a general-purpose solver for the initial-value problem (IVP) for systems of ordinary differential equations (ODEs). CVODE is written is ANSI-standard C and is based on two packages written in Fortran: VODE and VODPK. VODE is a general-purpose solver that includes methods for stiff and nonstiff system, and in the stiff case uses only direct methods for the solution of linear systems. VODPK is variant of VODE that uses a preconditioned GMRES method for the solution of linear systems. The basic methods represented in CVODE are presented. Two linear multistep methods are available: Adams methods in variable order, variable step form, and backward-differentiation-formula methods, in variable order, variable step, fixed-leading-coefficient.},
   author = {Scott D Cohen and Alan C Hindmarsh and Paul F Dubois},
   doi = {10.1063/1.4822377},
   issue = {2},
   journal = {Gas Mixtures The Journal of Chemical Physics},
   pages = {517},
   publisher = {American Inst of Physics},
   title = {CVODE, A Stiff/Nonstiff ODE Solver in C},
   volume = {10},
   url = {https://aip.scitation.org/toc/cip/10/2},
   year = {1996},
}
@report{Serban2003,
   abstract = {CVODES, which is part of the SUNDIALS software suite, is a stiff and nonstiff ordinary differential equation initial value problem solver with sensitivity analysis capabilities. CVODES is written in a data-independent manner, with a highly modular structure to allow incorporation of different preconditioning and/or linear solver methods. It shares with the other SUNDIALS solvers several common modules, most notably the generic kernel of vector operations and a set of generic linear solvers and preconditioners. CVODES solves the IVP by one of two methods-backward differentiation formula or Adams-Moulton-both implemented in a variable-step, variable-order form. The forward sensitivity module in CVODES implements the simultaneous corrector method, as well as two ¤avors of staggered corrector methods. Its adjoint sensitivity module provides a combination of check-pointing and cubic Hermite interpolation for the ef£cient generation of the forward solution during the adjoint system integration. We describe the current capabilities of CVODES, its design principles, and its user interface, and provide an example problem to illustrate the performance of CVODES.},
   author = {Radu Serban and Alan C Hindmash},
   title = {DETC2005-85597 CVODES, THE SENSITIVITY-ENABLED ODE SOLVER IN SUNDIALS *},
   url = {https://computing.llnl.gov/casc/nsde/pubs/toms_cvodes_with_covers.pdf},
   year = {2003},
}
@article{,
   doi = {10.1080/08958370701866008},
   title = {Health Effects of Organic Aerosols},
   url = {http://www.narsto.org/event.src?ID},
   year = {2014},
}
@generic{,
   abstract = {Aerosols are of central importance for atmospheric chemistry and physics, the biosphere, climate, and public health. The airborne solid and liquid particles in the nanometer to micrometer size range influence the energy balance of the Earth, the hydrological cycle, atmospheric circulation, and the abundance of greenhouse and reactive trace gases. Moreover, they play important roles in the reproduction of biological organisms and can cause or enhance diseases. The primary parameters that determine the environmental and health effects of aerosol particles are their concentration, size, structure, and chemical composition. These parameters, however, are spatially and temporally highly variable. The quantification and identification of biological particles and carbonaceous components of fine particulate matter in the air (organic compounds and black or elemental carbon, respectively) represent demanding analytical challenges. This Review outlines the current state of knowledge, major open questions, and research perspectives on the properties and interactions of atmospheric aerosols and their effects on climate and human health. © 2005 Wiley-VCH Verlag GmbH & Co. KGaA.},
   author = {Ulrich Pöschl},
   doi = {10.1002/anie.200501122},
   issn = {14337851},
   issue = {46},
   journal = {Angewandte Chemie - International Edition},
   keywords = {Aerosol particles,Atmospheric chemistry,Carbon,Reaction mechanisms,Water},
   month = {11},
   pages = {7520-7540},
   title = {Atmospheric aerosols: Composition, transformation, climate and health effects},
   volume = {44},
   year = {2005},
}
@article{Mitchell1971,
   abstract = {A generalized model of the effect of an optically thin atmospheric aerosol on the terrestrial heat budget is proposed, and applied to the problem of estimating the impact of the aerosol on temperatures near the earth's surface. The distinction between warming and cooling near the surface attributable to the aerosol is found on the basis of this model to depend on whether the ratio of absorption a to backscatter b of incoming solar radiation by the aerosol is greater or less than the critical border="0" border="0" SRC="/images/indent.gif" WIDTH=15 HEIGHT=10 BORDER=0 ALT=""&gt;where A is the surface albedo, C the fraction of sensible to total (sensible plus latent) solar heating of the surface, D the fraction of aerosol that is in convective contact with the surface, and k a multiple of b that measures the relative aerosol backscattering efficiency with respect to solar radiation reflected upward from the surface.&lt;IMG SRC="/images/indent.gif" WIDTH=15 HEIGHT=10 BORDER=0 ALT=""&gt;A distinction is drawn between a stratospheric aerosol (D=0) which generally cools the atmosphere near the surface, and a tropospheric aerosol (D&lt;IMG border="0" which may either cool or warm the atmosphere near the surface depending on various properties of the aerosol and of the surface itself. Over moist surfaces, such as vegetated areas and oceans, the critical ratio (a/b)o is of order 0.1. Over drier surfaces, such as deserts and urban areas, (a/b)o is of order unity. If the actual ratio a/b of most tropospheric aerosols is of order unity, as inferred by previous authors, then the dominant effect of such aerosols is warming except over deserts and urban arms where the effect is somewhat marginal between warming and cooling.&lt;IMG SRC="/images/indent.gif" WIDTH=15 HEIGHT=10 BORDER=0 ALT=""&gt;Further aerosol climatic effects are found likely to include a slight decrease of cloudiness and precipitation, and an increase of `planetary' albedo above the oceans, although not necessarily above the continents. Suggestions by several previous authors to the effect that the apparent worldwide cooling of climate in recent decades is attributable to large-scale increases of particulate pollution of the atmosphere by human activities are not supported by this analysis.},
   author = {J. Murray Mitchell},
   doi = {10.1175/1520-0450(1971)010<0703:teoaao>2.0.co;2},
   issn = {0021-8952},
   issue = {4},
   journal = {Journal of Applied Meteorology},
   month = {8},
   pages = {703-714},
   publisher = {American Meteorological Society},
   title = {The Effect of Atmospheric Aerosols on Climate with Special Reference to Temperature near the Earth's Surface},
   volume = {10},
   year = {1971},
}
@article{,
   author = {SS Brown and J Stutz - Chemical Society Reviews and undefined 2012},
   journal = {pubs.rsc.org},
   title = {Nighttime radical observations and chemistry},
   url = {https://pubs.rsc.org/en/content/articlehtml/2012/cs/c2cs35181a},
}
@article{,
   author = {R Atkinson and J Arey - Atmospheric Environment and undefined 2003},
   journal = {Elsevier},
   title = {Gas-phase tropospheric chemistry of biogenic volatile organic compounds: a review},
   url = {https://www.sciencedirect.com/science/article/pii/S1352231003003911},
}
@article{,
   author = {D Johnson and G Marston - Chemical Society Reviews and undefined 2008},
   journal = {pubs.rsc.org},
   title = {The gas-phase ozonolysis of unsaturated volatile organic compounds in the troposphere},
   url = {https://pubs.rsc.org/en/content/articlehtml/2008/cs/b704260b},
}
@report{Atkinson2003,
   abstract = {of the gas-phase reactions of OH radicals with alkanes and cycloalkanes. Abstract The available database concerning rate constants for gas-phase reactions of the hy-droxyl (OH) radical with alkanes through early 2003 is presented ove the entire temperature range for which measurements have been made (∼180-2000 K). Measurements made using relative rate methods are re-evaluated using recent rate data for the refer-5 ence compound (generally recommendations from this review). In general, whenever more than one study has been carried out over an overlapping temperature range, recommended rate constants or temperature-dependent rate expressions are presented.},
   author = {R Atkinson and R Atkinson Kinetics},
   issue = {4},
   journal = {Atmos. Chem. Phys. Discuss},
   pages = {4183-4358},
   title = {Kinetics of the gas-phase reactions of OH radicals with alkanes and cycloalkanes Kinetics of the gas-phase reactions},
   volume = {3},
   url = {www.atmos-chem-phys.org/acpd/3/4183/},
   year = {2003},
}
@article{,
   author = {RC Chapleski and Y Zhang and D Troya - Chemical Society Reviews and undefined 2016},
   journal = {pubs.rsc.org},
   title = {Heterogeneous chemistry and reaction dynamics of the atmospheric oxidants, O 3, NO 3, and OH, on organic surfaces},
   url = {https://pubs.rsc.org/--/content/articlehtml/2016/cs/c5cs00375j},
}
@article{Tilmes2016,
   abstract = {The Community Earth System Model, CESM1 CAM4-chem has been used to perform the Chemistry Climate Model Initiative (CCMI) reference and sensitivity simulations. In this model, the Community Atmospheric Model Version 4 (CAM4) is fully coupled to tropospheric and stratospheric chemistry. Details and specifics of each configuration, including new developments and improvements are described. CESM1 CAM4-chem is a low top model that reaches up to approximately 40 km and uses a horizontal resolution of 1.9 • latitude and 2.5 • longitude. For the specified dynamics experiments, the 5 model is nudged to Modern-Era Retrospective Analysis For Research And Applications (MERRA) reanalysis. We summarize the performance of the three reference simulations suggested by CCMI, with a focus on the observed period. Comparisons with elected datasets are employed to demonstrate the general performance of the model. We highlight new datasets that are suited for multi-model evaluation studies. Most important improvements of the model are the treatment of stratospheric aerosols and the corresponding adjustments for radiation and optics, the updated chemistry scheme including improved polar 10 chemistry and stratospheric dynamics, and improved dry deposition rates. These updates lead to a very good representation of tropospheric ozone within 20% of values from available observations for most regions. In particular, the trend and magnitude of surface ozone has been much improved compared to earlier versions of the model. Furthermore, stratospheric column ozone of the Southern Hemisphere in winter and spring is reasonably well represented. All experiments still underestimate CO most significantly in Northern Hemisphere spring and show a significant underestimation of hydrocarbons based on surface 15 observations.},
   author = {S Tilmes and J.-F Lamarque and L K Emmons and D E Kinnison and D Marsh and R R Garcia and A K Smith and R R Neely and A Conley and F Vitt and M Val Martin and H Tanimoto and I Simpson and D R Blake and N Blake},
   doi = {10.5194/gmd-2015-237},
   journal = {Geosci. Model Dev. Discuss},
   title = {Representation of the Community Earth System Model (CESM1) CAM4-chem within the Chemistry-Climate Model Initiative (CCMI)},
   url = {http://gmao.gsfc.nasa.gov/merra/},
   year = {2016},
}
@article{Lamarque2013,
   abstract = {The Atmospheric Chemistry and Climate Model Intercomparison Project (ACCMIP) consists of a series of time slice experiments targeting the long-term changes in atmospheric composition between 1850 and 2100, with the goal of documenting composition changes and the associated radiative forcing. In this overview paper, we introduce the ACCMIP activity, the various simulations performed (with a requested set of 14) and the associated model output. The 16 ACCMIP models have a wide range of horizontal and vertical resolutions, vertical extent, chemistry schemes and Published by Copernicus Publications on behalf of the European Geosciences Union. 180},
   author = {J.-F Lamarque and D T Shindell and B Josse and P J Young and I Cionni and V Eyring and D Bergmann and P Cameron-Smith and W J Collins and R Doherty and S Dalsoren and G Faluvegi and G Folberth and S J Ghan and L W Horowitz and Y H Lee and I A Mackenzie and T Nagashima and V Naik and D Plummer and M Righi and S T Rumbold and M Schulz and R B Skeie and D S Stevenson and S Strode and K Sudo and S Szopa and A Voulgarakis and G Zeng},
   doi = {10.5194/gmd-6-179-2013},
   journal = {Geosci. Model Dev},
   pages = {179-206},
   title = {The Atmospheric Chemistry and Climate Model Intercomparison Project (ACCMIP): overview and description of models, simulations and climate diagnostics},
   volume = {6},
   url = {www.geosci-model-dev.net/6/179/2013/},
   year = {2013},
}
@article{Dameris2013,
   abstract = {This paper reviews the current state and development of different numerical model classes that are used to simulate the global atmospheric system, particularly Earth's climate and climate-chemistry connections. The focus is on Chemistry-Climate Models. In general, these serve to examine dynamical and chemical processes in the Earth atmosphere, their feedback, and interaction with climate. Such models have been established as helpful tools in addition to analyses of observational data. Definitions of the global model classes are given and their capabilities as well as weaknesses are discussed. Examples of scientific studies indicate how numerical exercises contribute to an improved understanding of atmospheric behavior. There, the focus is on synergistic investigations combining observations and model results. The possible future developments and challenges are presented, not only from the scientific point of view but also regarding the computer technology and respective consequences for numerical modeling of atmospheric processes. In the future, a stronger cross-linkage of subject-specific scientists is necessary, to tackle the looming challenges. It should link the specialist discipline and applied computer science.},
   author = {Martin Dameris and Patrick Jöckel},
   doi = {10.3390/atmos4020132},
   issn = {2073-4433},
   journal = {Atmosphere},
   keywords = {Earth-System Model,atmospheric circulation,climate change,future projection,high-performance computing,ozone layer,ozone-climate connection,stratosphere,stratospheric water vapor,troposphere},
   pages = {132-156},
   title = {Numerical Modeling of Climate-Chemistry Connections: Recent Developments and Future Challenges},
   volume = {4},
   url = {www.mdpi.com/journal/atmosphere},
   year = {2013},
}
@article{Sun2018,
   abstract = {Global chemistry-climate models are computationally burdened as the chemical mechanisms become more complex and realistic. Optimization for graphics processing units (GPU) may make longer global simulation with regional detail possible, but limited study has been done to explore the potential benefit for the atmospheric chemistry modeling. Hence, in this study, the second-order Rosenbrock solver of the chemistry module of CAM4-Chem is ported to the GPU to gauge potential speed-up. We find that on the CPU, the fastest performance is achieved using the Intel compiler with a block interleaved memory layout. Different combinations of compiler and memory layout lead to ~11.02× difference in the computational time. In contrast, the GPU version performs the best when using a combination of fully interleaved memory layout with block size equal to the warp size, CUDA streams for independent kernels, and constant memory. Moreover, the most efficient data transfer between CPU and GPU is gained by allocating the memory contiguously during the data initialization on the GPU. Compared to one CPU core, the speed-up of using one GPU alone reaches a factor of ~11.7× for the computation alone and ~3.82× when the data transfer between CPU and GPU is considered. Using one GPU alone is also generally faster than the multithreaded implementation for 16 CPU cores in a compute node and the single-source solution (OpenACC). The best performance is achieved by the implementation of the hybrid CPU/GPU version, but rescheduling the workload among the CPU cores is required before the practical CAM4-Chem simulation.},
   author = {Jian Sun and Joshua S. Fu and John B. Drake and Qingzhao Zhu and Azzam Haidar and Mark Gates and Stanimire Tomov and Jack Dongarra},
   doi = {10.1029/2018MS001276},
   issn = {19422466},
   issue = {8},
   journal = {Journal of Advances in Modeling Earth Systems},
   keywords = {CUDA,GPU,compiler,data transfer,hybrid,memory layout},
   month = {8},
   pages = {1952-1969},
   publisher = {Blackwell Publishing Ltd},
   title = {Computational Benefit of GPU Optimization for the Atmospheric Chemistry Modeling},
   volume = {10},
   year = {2018},
}
@generic{Goodwin2018,
   author = {David G Goodwin and Raymond L Speth and Harry K Moffat and Bryan W Weber},
   doi = {10.5281/zenodo.1174508},
   note = {Version 2.4.0},
   title = {Cantera: An Object-oriented Software Toolkit for Chemical Kinetics, Thermodynamics, and Transport Processes},
   year = {2018},
}
@generic{Kroll2008,
   abstract = {Secondary organic aerosol (SOA), particulate matter composed of compounds formed from the atmospheric transformation of organic species, accounts for a substantial fraction of tropospheric aerosol. The formation of low-volatility (semivolatile and possibly nonvolatile) compounds that make up SOA is governed by a complex series of reactions of a large number of organic species, so the experimental characterization and theoretical description of SOA formation presents a substantial challenge. In this review we outline what is known about the chemistry of formation and continuing transformation of low-volatility species in the atmosphere. The primary focus is chemical processes that can change the volatility of organic compounds: (1) oxidation reactions in the gas phase, (2) reactions in the particle phase, and (3) continuing chemistry (in either phase) over several generations. Gas-phase oxidation reactions can reduce volatility by the addition of polar functional groups or increase it by the cleavage of carbon-carbon bonds; key branch points that control volatility are the initial attack of the oxidant, reactions of alkylperoxy (RO2) radicals, and reactions of alkoxy (RO) radicals. Reactions in the particle phase include oxidation reactions as well as accretion reactions, non-oxidative processes leading to the formation of high-molecular-weight species. Organic carbon in the atmosphere is continually subject to reactions in the gas and particle phases throughout its atmospheric lifetime (until lost by physical deposition or oxidized to CO or CO2), implying continual changes in volatility over the timescales of several days. The volatility changes arising from these chemical reactions must be parameterized and included in models in order to gain a quantitative and predictive understanding of SOA formation. © 2008 Elsevier Ltd. All rights reserved.},
   author = {Jesse H. Kroll and John H. Seinfeld},
   doi = {10.1016/j.atmosenv.2008.01.003},
   issn = {13522310},
   journal = {Atmospheric Environment},
   keywords = {Biogenic hydrocarbons,Particle-phase reactions,Secondary organic aerosol,Semivolatile organic compounds},
   title = {Chemistry of secondary organic aerosol: Formation and evolution of low-volatility organics in the atmosphere},
   year = {2008},
}
@article{Chen2019,
   abstract = {We apply a high-resolution chemical transport model (GEOS-Chem CTM) with updated treatment of volatile organic compounds (VOCs) and a comprehensive suite of airborne datasets over North America to (i) characterize the VOC budget and (ii) test the ability of current models to capture the distribution and reactivity of atmospheric VOCs over this region. Biogenic emissions dominate the North American VOC budget in the model, accounting for 70&thinsp;% and 95&thinsp;% of annually emitted VOC carbon and reactivity, respectively. Based on current inventories anthropogenic emissions have declined to the point where biogenic emissions are the dominant summertime source of VOC reactivity even in most major North American cities. Methane oxidation is a 2<span classCombining double low line"inline-formula">×</span> larger source of nonmethane VOCs (via production of formaldehyde and methyl hydroperoxide) over North America in the model than are anthropogenic emissions. However, anthropogenic VOCs account for over half of the ambient VOC loading over the majority of the region owing to their longer aggregate lifetime. Fires can be a significant VOC source episodically but are small on average. In<span idCombining double low line"page9098"/> the planetary boundary layer (PBL), the model exhibits skill in capturing observed variability in total VOC abundance (<span classCombining double low line"inline-formula"><i>R</i>2Combining double low line0.36</span>) and reactivity (<span classCombining double low line"inline-formula"><i>R</i>2Combining double low line0.54</span>). The same is not true in the free troposphere (FT), where skill is low and there is a persistent low model bias (<span classCombining double low line"inline-formula">ĝ1/4</span>&thinsp;60&thinsp;%), with most (27 of 34) model VOCs underestimated by more than a factor of 2. A comparison of PBL&thinsp;:&thinsp;FT concentration ratios over the southeastern US points to a misrepresentation of PBL ventilation as a contributor to these model FT biases. We also find that a relatively small number of VOCs (acetone, methanol, ethane, acetaldehyde, formaldehyde, isoprene <span classCombining double low line"inline-formula">+</span> oxidation products, methyl hydroperoxide) drive a large fraction of total ambient VOC reactivity and associated model biases; research to improve understanding of their budgets is thus warranted. A source tracer analysis suggests a current overestimate of biogenic sources for hydroxyacetone, methyl ethyl ketone and glyoxal, an underestimate of biogenic formic acid sources, and an underestimate of peroxyacetic acid production across biogenic and anthropogenic precursors. Future work to improve model representations of vertical transport and to address the VOC biases discussed are needed to advance predictions of ozone and SOA formation.},
   author = {Xin Chen and Dylan B. Millet and Hanwant B. Singh and Armin Wisthaler and Eric C. Apel and Elliot L. Atlas and Donald R. Blake and Ilann Bourgeois and Steven S. Brown and John D. Crounse and Joost A. De Gouw and Frank M. Flocke and Alan Fried and Brian G. Heikes and Rebecca S. Hornbrook and Tomas Mikoviny and Kyung Eun Min and Markus Müller and J. Andrew Neuman and Daniel W. O'sullivan and Jeff Peischl and Gabriele G. Pfister and Dirk Richter and James M. Roberts and Thomas B. Ryerson and Stephen R. Shertz and Chelsea R. Thompson and Victoria Treadaway and Patrick R. Veres and James Walega and Carsten Warneke and Rebecca A. Washenfelder and Petter Weibring and Bin Yuan},
   doi = {10.5194/acp-19-9097-2019},
   issn = {16807324},
   issue = {14},
   journal = {Atmospheric Chemistry and Physics},
   pages = {9097-9123},
   title = {On the sources and sinks of atmospheric VOCs: an integrated analysis of recent aircraft campaigns over North America},
   volume = {19},
   year = {2019},
}
@article{Ammann2007,
   abstract = {A kinetic model framework with consistent and unambiguous terminology and universally applicable rate equations and parameters for aerosol and cloud surface chemistry and gas-particle interactions has been presented in the preceding companion paper by Pöschl, Rudich and Ammann (Pöschl et al., 2007), abbreviated PRA. It allows to describe mass transport and chemical reaction at the gas-particle interface and to link aerosol and cloud surface processes with gas phase and particle bulk processes. Here we present multiple exemplary model systems and calculations illustrating how the general mass balance and rate equations of the PRA framework can be easily reduced to compact sets of equations which enable a mechanistic description of time and concentration dependencies of trace gas uptake and particle composition in systems with one or more chemical components and physicochemical processes. Time-dependent model scenarios show the effects of reversible adsorption, surface-bulk transport, and chemical aging on the temporal evolution of trace gas uptake by solid particles and solubility saturation of liquid particles. They demonstrate how the transformation of particles and the variation of trace gas accommodation and uptake coefficients by orders of magnitude over time scales of microseconds to days can be explained and predicted from the initial composition and basic kinetic parameters of model systems by iterative calculations using standard spreadsheet programs. Moreover, they show how apparently inconsistent experimental data sets obtained with different techniques and on different time scales can be efficiently linked and mechanistically explained by application of consistent model formalisms and terminologies within the PRA framework. Steady-state model scenarios illustrate characteristic effects of gas phase composition and basic kinetic parameters on the rates of mass transport and chemical reactions. They demonstrate how adsorption and surface saturation effects can explain non-linear gas phase concentration dependencies of surface and bulk accommodation coefficients, uptake coefficients, and bulk solubilities (deviations from Henry's law). Such effects are expected to play an important role in many real atmospheric aerosol and cloud systems involving a wide range of organic and inorganic components of concentrated aqueous and organic solution droplets, ice crystals, and other crystalline or amorphous solid particles.},
   author = {M. Ammann and U. Pöschl},
   doi = {10.5194/acp-7-6025-2007},
   issn = {16807324},
   issue = {23},
   journal = {Atmospheric Chemistry and Physics},
   pages = {6025-6045},
   title = {Kinetic model framework for aerosol and cloud surface chemistry and gas-particle interactions - Part 2: Exemplary practical applications and numerical simulations},
   volume = {7},
   year = {2007},
}
@article{Riemer2019,
   abstract = {Atmospheric aerosols are complex mixtures of different chemical species, and individual particles exist in many different shapes and morphologies. Together, these characteristics contribute to the aerosol mixing state. This review provides an overview of measurement techniques to probe aerosol mixing state, discusses how aerosol mixing state is represented in atmospheric models at different scales, and synthesizes our knowledge of aerosol mixing state's impact on climate-relevant properties, such as cloud condensation and ice nucleating particle concentrations, and aerosol optical properties. We present these findings within a framework that defines aerosol mixing state along with appropriate mixing state metrics to quantify it. Future research directions are identified, with a focus on the need for integrating mixing state measurements and modeling.},
   author = {N. Riemer and A. P. Ault and M. West and R. L. Craig and J. H. Curtis},
   doi = {10.1029/2018RG000615},
   issn = {19449208},
   issue = {2},
   journal = {Reviews of Geophysics},
   keywords = {aerosol climate impacts,aerosol measurements,aerosol mixing state,aerosol modeling},
   pages = {187-249},
   title = {Aerosol Mixing State: Measurements, Modeling, and Impacts},
   volume = {57},
   year = {2019},
}
@article{Xu2019,
   abstract = {It is important to study the correlation between fossil fuel combustion products and atmospheric environmental pollution for improving atmospheric environmental quality. Therefore, the correlation between fossil fuel combustion products and atmospheric environmental pollution is constructed. Samples of NO 2 , O 3 , CO, SO 2 and PM2.5 in a city are collected. Multiple linear regression models of correlation among NO 2 , SO 2 and PM2.5, and regression models of correlation among O 3 , CO and PM2.5 are constructed according to the results of sample analysis. The results show that NO 2, SO 2 and PM2.5 have strong correlation, the correlation between day and night in summer is slightly better than that of winter, and the correlation between winter and summer is better than that of summer. In addition, the correlation regression model of O 3 , CO and PM2.5 can basically estimate the change trend of PM2.5 particle concentration. The results show that the model can provide a reliable means for analyzing the correlation between fossil fuel combustion products and atmospheric environmental pollution, and play an important role in improving environmental quality.},
   author = {Juan Xu},
   issn = {13001361},
   issue = {107},
   journal = {Ekoloji},
   keywords = {Atmospheric environmental pollution,Combustion products,Correlation,Fossil fuels,Modeling,PM2.5},
   pages = {2255-2263},
   title = {Modeling of correlation between fossil fuel combustion products and atmospheric environmental pollution},
   volume = {28},
   year = {2019},
}
@article{Guo2020,
   abstract = {The reaction with chlorine radicals (·Cl) has been considered to be one of indispensable sinks for isoprene. However, the mechanism of ·Cl initiated isoprene reaction was not fully understood. Herein, the reaction of isoprene with ·Cl, and ensuing reactions of the resulting isoprene relevant radicals were investigated by combined quantum chemistry calculations and kinetics modeling. The results indicate that ·Cl addition to two terminal C-atoms of two double bonds of isoprene, forming IM1–1 and IM1–4, are more favorable than H-abstractions from isoprene. Interestingly, the predicted reaction rate constant for the direct H-abstraction pathway is much lower than that of the indirect one, clarifying a direct H-abstraction mechanism for previously experimental observation. The IM1–1 and IM1–4 have distinct fate in their subsequent transformation. The reaction of IM1–1 ends after the one-time O2 addition. However, IM1–4 can follow auto-oxidation mechanism with two times O2 addition to finally form highly oxidized multi-functional molecules (HOMs), C5H7ClO3 and ·OH. More importantly, the estimated contribution of ·Cl on HOMs (monomer only) formation from isoprene is lower than that of ·OH in addition pathway, implying overall HOMs yield from atmospheric isoprene oxidation could be overestimated if the role of ·Cl in transforming isoprene is ignored.},
   author = {Xirui Guo and Fangfang Ma and Cong Liu and Junfeng Niu and Ning He and Jingwen Chen and Hong Bin Xie},
   doi = {10.1016/j.scitotenv.2019.136330},
   issn = {18791026},
   journal = {Science of the Total Environment},
   keywords = {HOMs,SOA formation,VOCs},
   pages = {136330},
   publisher = {Elsevier B.V.},
   title = {Atmospheric oxidation mechanism and kinetics of isoprene initiated by chlorine radicals: A computational study},
   volume = {712},
   url = {https://doi.org/10.1016/j.scitotenv.2019.136330},
   year = {2020},
}
@generic{Riemer2019,
   abstract = {Atmospheric aerosols are complex mixtures of different chemical species, and individual particles exist in many different shapes and morphologies. Together, these characteristics contribute to the aerosol mixing state. This review provides an overview of measurement techniques to probe aerosol mixing state, discusses how aerosol mixing state is represented in atmospheric models at different scales, and synthesizes our knowledge of aerosol mixing state's impact on climate-relevant properties, such as cloud condensation and ice nucleating particle concentrations, and aerosol optical properties. We present these findings within a framework that defines aerosol mixing state along with appropriate mixing state metrics to quantify it. Future research directions are identified, with a focus on the need for integrating mixing state measurements and modeling.},
   author = {N. Riemer and A. P. Ault and M. West and R. L. Craig and J. H. Curtis},
   doi = {10.1029/2018RG000615},
   issn = {19449208},
   issue = {2},
   journal = {Reviews of Geophysics},
   keywords = {aerosol climate impacts,aerosol measurements,aerosol mixing state,aerosol modeling},
   month = {6},
   pages = {187-249},
   publisher = {Blackwell Publishing Ltd},
   title = {Aerosol Mixing State: Measurements, Modeling, and Impacts},
   volume = {57},
   year = {2019},
}
@article{,
   author = {J Xu - Ekoloji Dergisi and undefined 2019},
   journal = {ekolojidergisi.com},
   title = {Modeling of Correlation between Fossil Fuel Combustion Products and Atmospheric Environmental Pollution.},
   url = {http://www.ekolojidergisi.com/download/modeling-of-correlation-between-fossil-fuel-combustion-products-and-atmospheric-environmental-5855.pdf},
}
@article{,
   abstract = {A new chemical mechanism for the oxidation of biogenic volatile organic compounds (BVOCs) is presented and implemented in the Model of Atmospheric composition at Global and Regional scales using Inversion Techniques for Trace gas Emissions (MAGRITTE v1.1). With a total of 105 organic species and over 265 gas-phase reactions, 69 pho-todissociations, and 7 heterogeneous reactions, the mechanism treats the chemical degradation of isoprene-its main focus-as well as acetaldehyde, acetone, methylbutenol, and the family of monoterpenes. Regarding isoprene, the mechanism incorporates a state-of-the-art representation of its oxidation scheme accounting for all major advances put forward in recent theoretical and laboratory studies. The recycling of OH radicals in isoprene oxidation through the isomer-ization of Z-δ-hydroxyperoxy radicals is found to enhance OH concentrations by up to 40 % over western Amazonia in the boundary layer and by 10 %-15 % over the southeastern US and Siberia in July. The model and its chemical mechanism are evaluated against the suite of chemical measurements from the SEAC 4 RS (Studies of Emissions and Atmospheric Composition, Clouds and Climate Coupling by Regional Surveys) airborne campaign, demonstrating a good overall agreement for major isoprene oxidation products, although the aerosol hydrolysis of tertiary and non-tertiary nitrates remain poorly constrained. The comparisons for methylnitrate indicate a very low nitrate yield (< 3 × 10 −4) in the CH 3 O 2 + NO reaction. The oxidation of isoprene, acetone, and acetaldehyde by OH is shown to be a substantial source of enols and keto-enols, primarily through the photolysis of multifunctional carbonyls generated in their oxidation schemes. Oxidation of those enols by OH radicals constitutes a sizable source of carboxylic acids estimated at 9 Tg (HC(O)OH) yr −1 and 11 Tg(CH 3 C(O)OH) yr −1 or ∼ 20 % of their global identified source. The ozonolysis of alkenes is found to be a smaller source of HC(O)OH (6 Tg HC(O)OH yr −1) than previously estimated, due to several factors including the strong deposition sink of hydrox-ymethyl hydroperoxide (HMHP).},
   author = {Jean-François Müller and Trissevgeni Stavrakou and Jozef Peeters},
   doi = {10.5194/gmd-12-2307-2019},
   journal = {Geosci. Model Dev},
   pages = {2307-2356},
   title = {Chemistry and deposition in the Model of Atmospheric composition at Global and Regional scales using Inversion Techniques for Trace gas Emissions (MAGRITTE v1.1)-Part 1: Chemical mechanism},
   volume = {12},
   url = {https://doi.org/10.5194/gmd-12-2307-2019},
   year = {2019},
}
@article{,
   doi = {10.5194/acp-19-9097-2019},
   title = {UC Irvine UC Irvine Previously Published Works Title},
   url = {https://doi.org/10.5194/acp-19-9097-2019},
}
@article{,
   author = {C Wang and F Wania and KU Goss - Environmental Science: Processes & and undefined 2018},
   journal = {pubs.rsc.org},
   title = {Is secondary organic aerosol yield governed by kinetic factors rather than equilibrium partitioning?},
   url = {https://pubs.rsc.org/en/content/articlehtml/2018/em/c7em00451f},
}
@article{,
   abstract = {Environmental chambers have proven to be essential for atmospheric photochemistry research. This historical perspective summarizes chamber research characterizing smog. Experiments with volatile organic compounds (VOCs)-nitrogen oxides (NO x) have characterized O 3 and aerosol chemistry. These led to the creation and evaluation of complex reaction mechanisms adopted for various applications. Gas-phase photochemistry was initiated and developed using chamber studies. Post-1950s study of photochemical aerosols began using smog chambers. Much of the knowledge about the chemistry of secondary organic aerosols (SOA) derives from chamber studies complemented with specially designed atmospheric studies. Two major findings emerge from post-1990s SOA experiments: (1) photochemical SOAs hypothetically involve hydrocarbons and oxygenates with carbon numbers of 2, and (2) SOA evolves via more than one generation of reactions as condensed material exchanges with the vapor phase during "aging". These elements combine with multiphase chemistry to yield mechanisms for aerosols. Smog chambers, like all simulators, are limited representations of the atmosphere. Translation to the atmosphere is complicated by constraints in reaction times, container interactions, influence of precursor injections, and background species. Interpretation of kinetics requires integration into atmospheric models addressing the combined effects of precursor emissions, surface exchange, hydrometeor interactions, air motion and sunlight.},
   author = {G M Hidy},
   doi = {10.3390/atmos10070401},
   keywords = {atmospheric simulation,ozone,photochemical aerosols,reactivity,smog chambers},
   title = {Atmospheric Chemistry in a Box or a Bag},
   url = {www.mdpi.com/journal/atmosphere},
}
@article{Wallington2018,
   abstract = {AbstractRemarkable progress has occurred over the last 100 years in our understanding of atmospheric chemical composition, stratospheric and tropospheric chemistry, urban air pollution, acid rain, and the formation of airborne particles from gas-phase chemistry. Much of this progress was associated with the developing understanding of the formation and role of ozone and of the oxides of nitrogen, NO and NO2, in the stratosphere and troposphere. The chemistry of the stratosphere, emerging from the pioneering work of Chapman in 1931, was followed by the discovery of catalytic ozone cycles, ozone destruction by chlorofluorocarbons, and the polar ozone holes, work honored by the 1995 Nobel Prize in Chemistry awarded to Crutzen, Rowland, and Molina. Foundations for the modern understanding of tropospheric chemistry were laid in the 1950s and 1960s, stimulated by the eye-stinging smog in Los Angeles. The importance of the hydroxyl (OH) radical and its relationship to the oxides of nitrogen (NO and NO2) emerged. The chemical processes leading to acid rain were elucidated. The atmosphere contains an immense number of gas-phase organic compounds, a result of emissions from plants and animals, natural and anthropogenic combustion processes, emissions from oceans, and from the atmospheric oxidation of organics emitted into the atmosphere. Organic atmospheric particulate matter arises largely as gas-phase organic compounds undergo oxidation to yield low-volatility products that condense into the particle phase. A hundred years ago, quantitative theories of chemical reaction rates were nonexistent. Today, comprehensive computer codes are available for performing detailed calculations of chemical reaction rates and mechanisms for atmospheric reactions. Understanding the future role of atmospheric chemistry in climate change and, in turn, the impact of climate change on atmospheric chemistry will be critical to developing effective policies to protect the planet.},
   author = {T. J. Wallington and J. H. Seinfeld and J. R. Barker},
   doi = {10.1175/amsmonographs-d-18-0008.1},
   issn = {0065-9401},
   journal = {Meteorological Monographs},
   month = {1},
   pages = {10.1-10.52},
   publisher = {American Meteorological Society},
   title = {100 Years of Progress in Gas-Phase Atmospheric Chemistry Research},
   volume = {59},
   year = {2018},
}
@article{Moridnejad2016,
   abstract = {Isotopic exchange experiments that utilize D2O and H2O have received attention as a method for studying water diffusion in high viscosity aerosol particles. However, the mathematical models used to retrieve diffusion coefficients from these measurements have yet to be critically examined. Here, two models for the isotopic exchange of D2O and H2O in spherical particles are analyzed and compared. The primary difference between the two models is the choice of boundary condition at the surface of the spherical particle. In one model, it is assumed that the concentration of D2O at the surface is fixed, while in the other model, it is assumed that, at the particle surface, the concentration of D2O in the condensed phase is in equilibrium with D2O vapor. Closed-form expressions for the two boundary value problems that describe these physical models are found and discussed. Then, specific examples of aqueous droplets containing either sucrose, citric acid, and shikimic acid are examined with both models. It is found that at low relative humidities the choice of boundary condition has a negligible effect on the predicted lifetime of isotopic exchange, while at high relative humidities predicted lifetimes can differ by orders of magnitude. The implication of this result is that the choice of model can greatly affect diffusion coefficients retrieved from experimental measurements under certain conditions. Finally, discrepancies between diffusion coefficients measured using isotopic exchange and water sorption and desorption experiments are discussed.},
   author = {Ali Moridnejad and Thomas C. Preston},
   doi = {10.1021/acs.jpca.6b11241},
   issn = {15205215},
   issue = {49},
   journal = {Journal of Physical Chemistry A},
   pages = {9759-9766},
   publisher = {American Chemical Society},
   title = {Models of isotopic water diffusion in spherical aerosol particles},
   volume = {120},
   year = {2016},
}
@generic{Ault2017,
   author = {Andrew P. Ault and Jessica L. Axson},
   doi = {10.1021/acs.analchem.6b04670},
   issn = {15206882},
   issue = {1},
   journal = {Analytical Chemistry},
   month = {1},
   pages = {430-452},
   publisher = {American Chemical Society},
   title = {Atmospheric Aerosol Chemistry: Spectroscopic and Microscopic Advances},
   volume = {89},
   year = {2017},
}
@article{Karadima2019,
   abstract = {We explore the morphologies of multicomponent nanoparticles through atomistic molecular dynamics simulations under atmospherically relevant conditions. The particles investigated consist of both organic (cis-pinonic acid-CPA, 3-methyl-1,2,3-butanetricarboxylic acid-MBTCA, n-C 20 H 42 , n-C 24 H 50 , n-C 30 H 62 or mixtures thereof) and inorganic (sulfate, ammonium and water) compounds. The effects of relative humidity, organic mass content and type of organic compound present in the nanoparticle are investigated. Phase separation is predicted for almost all simulated nanoparticles either between organics and inorganics or between hydrophobic and hydrophilic constituents. For oxy-genated organics, our simulations predict an enrichment of the nanoparticle surface in organics, often in the form of islands depending on the level of humidity and organic mass fraction, giving rise to core-shell structures. In several cases the organics separate from the inorganics, especially from the ions. For particles containing water-insoluble linear alkanes, separate hydrophobic and hydrophilic domains are predicted to develop. The surface partitioning of organics is enhanced as the humidity increases. The presence of organics in the interior of the nanoparticle increases as their overall mass fraction in the nanoparticle increases, but this also depends on the humidity conditions. Apart from the organics-inorganics and hydrophobics-hydrophilics separation, our simulations predict a third type of separation (layering) between CPA and MBTCA molecules under certain conditions.},
   author = {Katerina S Karadima and Vlasis G Mavrantzas and Spyros N Pandis},
   doi = {10.3929/ethz-b-000341303},
   issue = {8},
   journal = {Atmospheric Chemistry and Physics},
   pages = {5571-5587},
   title = {Insights into the morphology of multicomponent organic and inorganic aerosols from molecular dynamics simulations ETH Library Insights into the morphology of multicomponent organic and inorganic aerosols from molecular dynamics simulations},
   volume = {19},
   url = {http://doi.org/10.5194/acp-19-5571-2019},
   year = {2019},
}
@report{Springmann2009,
   abstract = {This study assesses in detail the effects of heterogeneous chemistry on the particle surface and gas-phase composition by modeling the reversible co-adsorption of O3, NO2, and H2O on soot coated with benzo[a]pyrene (BaP) for an urban plume scenario over a period of five days. By coupling the Pöschl-Rudich-Ammann (PRA) kinetic framework for aerosols (Pöschl et al., 2007) to a box model version of the gas phase mechanism RADM2, we are able to track individual concentrations of gas-phase and surface species over the course of several days. The flux-based PRA formulation takes into account changes in the uptake kinetics due to changes in the chemical gas-phase and particle surface compositions. This dynamic uptake coefficient approach is employed for the first time in a broader atmospheric context of an urban plume scenario. Our model scenarios include one to three adsorbents and three to five coupled surface reactions. The results show a variation of the O3 and NO2 uptake coefficients of more than five orders of magnitude over the course of the simulation time and a decrease in the uptake coefficients in the various scenarios by more than three orders of magnitude within the first six hours. Thereafter, periodic peaks of the uptake coefficients follow the diurnal cycle of gas-phase O 3-NOx reactions. Physisorption of water vapor reduces the half-life of the coating substance BaP by up to a factor of seven by permanently occupying ∼75% of the soot surface. Soot emissions modeled by replenishing reactive surface sites lead to maximum gas-phase O3 depletions of 41 ppbv and 7.8 ppbv for an hourly and six-hourly replenishment cycle, respectively. This conceptual study highlights the interdependence of co-adsorbing species and their nonlinear gas-phase feedback. It yields further insight into the atmospheric importance of the chemical oxidation of particles and emphasizes the necessity to implement detailed heterogeneous kinetics in future modeling studies.},
   author = {M. Springmann and D. A. Knopf and N. Riemer},
   doi = {10.5194/acp-9-7461-2009},
   issn = {16807324},
   issue = {19},
   journal = {Atmospheric Chemistry and Physics},
   pages = {7461-7479},
   title = {Detailed heterogeneous chemistry in an urban plume box model: Reversible co-adsorption of O3, NO2, and H2O on soot coated with benzo[a]pyrene},
   volume = {9},
   url = {www.atmos-chem-phys.net/9/7461/2009/},
   year = {2009},
}
@report{Ammann2005,
   abstract = {To cite this version: M. Ammann, U. Pöschl. Kinetic model framework for aerosol and cloud surface chemistry and gas-particle interactions: Part 2 ? exemplary practical applications and numerical simulations.},
   author = {M Ammann and U Pöschl},
   issue = {2},
   journal = {Chemistry and Physics Discussions},
   pages = {2193-2246},
   title = {Kinetic model framework for aerosol and cloud surface chemistry and gas-particle interactions: Part 2 ? exemplary practical applications and numerical simulations},
   volume = {5},
   url = {www.atmos-chem-phys.org/acpd/5/2193/},
   year = {2005},
}
@article{,
   author = {X Guo and F Ma and C Liu and J Niu and N He and J Chen - Science of The Total … and undefined 2020},
   journal = {Elsevier},
   title = {Atmospheric oxidation mechanism and kinetics of isoprene initiated by chlorine radicals: A computational study},
   url = {https://www.sciencedirect.com/science/article/pii/S0048969719363260},
}
@article{Nakayama2017,
   abstract = {As a new approach to investigating formation processes of secondary organic aerosol (SOA) in the atmosphere, ozone-induced potential aerosol formation was measured in summer at a suburban forest site surrounded by deciduous trees, near Tokyo, Japan. After passage through a reactor containing high concentrations of ozone, increases in total particle volume (average of 1.4 × 109 nm3/cm3, which corresponds to 17% that of pre-existing particles) were observed, especially during daytime. The observed aerosol formations were compared with the results of box model simulations using simultaneously measured concentrations of gaseous and particulate species. According to the model, the relative contributions of isoprene, monoterpene, and aromatic hydrocarbon oxidation to SOA formation in the reactor were 24, 21, and 55%, respectively. However, the model could explain, on average, only ∼40% of the observed particle formation, and large discrepancies between the observations and model were found, especially around noon and in the afternoon when the concentrations of isoprene and oxygenated volatile organic compounds were high. The results suggest a significant contribution of missing (unaccounted-for) SOA formation processes from identified and/or unidentified volatile organic compounds, especially those emitted during daytime. Further efforts should be made to explore and parameterize this missing SOA formation to assist in the improvement of atmospheric chemistry and climate models.},
   author = {T. Nakayama and Y. Kuruma and Y. Matsumi and Y. Morino and K. Sato and H. Tsurumaru and S. Ramasamy and Y. Sakamoto and S. Kato and Y. Miyazaki and T. Mochizuki and K. Kawamura and Y. Sadanaga and Y. Nakashima and K. Matsuda and Y. Kajii},
   doi = {10.1016/j.atmosenv.2017.10.014},
   issn = {18732844},
   journal = {Atmospheric Environment},
   keywords = {Flow reactor,Ozone,Potential aerosol formation,SOA,Suburban forest},
   title = {Missing ozone-induced potential aerosol formation in a suburban deciduous forest},
   year = {2017},
}
@article{Xie2017,
   abstract = {Among 160 organic NHx-containing compounds (x = 1, 2) detected in the atmosphere, there are about 80 species for which the molecules contain p-π conjugate substructures of NHx-π-bonds. Here, chlorine radical (·Cl)-initiated reactions for formamide, N-methylformamide, ethenamine, and aniline, as their cases, were investigated by a quantum chemical method [CCSD(T)/aug-cc-pVTZ//MP2/6-31+G(3df,2p)] and kinetics modeling. The calculated overall rate constants are 5.5 × 10-11, 2.3 × 10-10, 2.7 × 10-10, and 1.7 × 10-10 cm3 molecule-1 s-1 for formamide, N-methylformamide, ethenamine, and aniline, respectively, and agree well with experimental values for available ones. Importantly, the results show that the reactions of two amides with ·Cl mainly lead to C-center radicals via ·Cl abstracting the -CHO hydrogen of amides. However, both ethenamine + ·Cl and aniline + ·Cl reactions mainly produce delocalized radicals with the radical center on the C-site and N-site via a ·Cl addition and the -NHx hydrogen abstraction pathway, respectively. Therefore, this study reveals that reactions of organic NHx-containing compounds with ·Cl have various reaction mechanisms, in contrast to our previous understanding that -NHx hydrogen abstraction pathways, leading to N-center radicals, are the most favorable. The unveiled reaction mechanisms should be of significance for the risk assessment of atmospheric organic NHx-containing compounds and enriching ·Cl chemistry.},
   author = {Hong Bin Xie and Fangfang Ma and Qi Yu and Ning He and Jingwen Chen},
   doi = {10.1021/acs.jpca.6b11418},
   issn = {15205215},
   issue = {8},
   journal = {Journal of Physical Chemistry A},
   month = {3},
   pages = {1657-1665},
   publisher = {American Chemical Society},
   title = {Computational Study of the Reactions of Chlorine Radicals with Atmospheric Organic Compounds Featuring NHx-π-Bond (x = 1, 2) Structures},
   volume = {121},
   year = {2017},
}
@generic{Glasius2016,
   abstract = {Earth's atmosphere contains a multitude of organic compounds, which differ by orders of magnitude regarding fundamental properties such as volatility, reactivity, and propensity to form cloud droplets, affecting their impact on global climate and human health. Despite recent major research efforts and advances, there are still substantial gaps in understanding of atmospheric organic chemistry, hampering efforts to understand, model, and mitigate environmental problems such as aerosol formation in both polluted urban and more pristine regions. The analytical toolbox available for chemists to study atmospheric organic components has expanded considerably during the past decade, opening new windows into speciation, time resolution and detection of reactive and semivolatile compounds at low concentrations. This has provided unprecedented opportunities, but also unveiled new scientific challenges. Specific groundbreaking examples include the role of epoxides in aerosol formation especially from isoprene, the importance of highly oxidized, reactive organics in air-surface processes (whether atmosphere-biosphere exchange or aerosols), as well as the extent of interactions of anthropogenic and biogenic emissions and the resulting impact on atmospheric organic chemistry.},
   author = {Marianne Glasius and Allen H. Goldstein},
   doi = {10.1021/acs.est.5b05105},
   issn = {15205851},
   issue = {6},
   journal = {Environmental Science and Technology},
   month = {3},
   pages = {2754-2764},
   publisher = {American Chemical Society},
   title = {Recent Discoveries and Future Challenges in Atmospheric Organic Chemistry},
   volume = {50},
   year = {2016},
}
@generic{Shen2013,
   abstract = {Volatile organic compounds (VOCs) are of central importance in the atmosphere because of their close relation to air quality and climate change. As a significant sink for VOCs, the fate of VOCs via heterogeneous reactions may explain the big gap between field and model studies. These reactions play as yet unclear but potentially crucial role in atmospheric processes. In order to better evaluate this reaction pathway, we present the first specific review for the progress of heterogeneous reaction studies on VOCs, including carbonyl compounds, organic acids, alcohols, and so on. Our review focuses on the processes for heterogeneous reactions of VOCs under varying experimental conditions, as well as their implications for trace gas and HOx budget, secondary organic aerosol (SOA) formation, physicochemical properties of aerosols, and human health. Finally, we propose the future direction for laboratory studies of heterogeneous chemistry of VOCs that should be carried out under more atmospherically relevant conditions, with a special emphasis on the effects of relative humidity and illumination, the multicomponent reaction systems, and reactivity of aged and authentic particles. In particular, more reliable uptake coefficients, based on the abundant elaborate laboratory studies, appropriate calibration, and logical choice criterion, are urgently required in atmospheric models. © 2012 Elsevier Ltd.},
   author = {Xiaoli Shen and Yue Zhao and Zhongming Chen and Dao Huang},
   doi = {10.1016/j.atmosenv.2012.11.027},
   issn = {13522310},
   journal = {Atmospheric Environment},
   keywords = {Aerosol ageing,HOx budget,Heterogeneous reaction,SOA formation,Uptake coefficient,Volatile organic compounds},
   title = {Heterogeneous reactions of volatile organic compounds in the atmosphere},
   year = {2013},
}
@article{Atkinson2000,
   abstract = {The present status of knowledge of the gas-phase reactions of inorganic O(x), HO(x) and NO(x) species and of selected classes of volatile organic compounds (VOCs) [alkanes, alkenes, aromatic hydrocarbons, oxygen-containing VOCs and nitrogen-containing VOCs] and their degradation products in the troposphere is discussed. There is now a good qualitative and, in a number of areas, quantitative understanding of the tropospheric chemistry of NO(x) and VOCs involved in the photochemical formation of ozone. During the past five years much progress has been made in elucidating the reactions of alkoxy radicals, the mechanisms of the gas-phase reactions of O3 with alkenes, and the mechanisms and products of the OH radical-initiated reactions of aromatic hydrocarbons, and further progress is expected. However, there are still areas of uncertainty which impact the ability to accurately model the formation of ozone in urban, rural and regional areas, and these include a need for: rate constants and mechanisms of the reactions of organic peroxy (RO2) radicals with NO, NO3 radicals, HO2 radicals and other RO2 radicals; organic nitrate yields from the reactions of RO2 radicals with NO, preferably as a function of temperature and pressure; the reaction rates of alkoxy radicals for decomposition, isomerization, and reaction with O2, especially for alkoxy radicals other than those formed from alkanes and alkenes; the detailed mechanisms of the reactions of O3 with alkenes and VOCs containing >C=C< bonds; the mechanisms and products of the reactions of OH-aromatic adducts with O2 and NO2; the tropospheric chemistry of many oxygenated VOCs formed as first-generation products of VOC photooxidations; and a quantitative understanding of the reaction sequences leading to products which gas/particle partition and lead to secondary organic aerosol formation. Copyright (C) 2000 Elsevier Science Ltd.},
   author = {Roger Atkinson},
   doi = {10.1016/S1352-2310(99)00460-4},
   issn = {13522310},
   journal = {Atmospheric Environment},
   keywords = {Hydroxyl radical,Nitrate radical,Oxides of nitrogen,Ozone,Reaction kinetics,Reaction mechanisms,Tropospheric chemistry,Volatile organic compounds},
   title = {Atmospheric chemistry of VOCs and NO(x)},
   year = {2000},
}
@report{Springmann2009,
   abstract = {This study assesses in detail the effects of heterogeneous chemistry on the particle surface and gas-phase composition by modeling the reversible co-adsorption of O 3 , NO 2 , and H 2 O on soot coated with benzo[a]pyrene (BaP) for an urban plume scenario over a period of five days. By coupling the Pöschl-Rudich-Ammann (PRA) kinetic framework for aerosols (Pöschl et al., 2007) to a box model version of the gas phase mechanism RADM2, we are able to track individual concentrations of gas-phase and surface species over the course of several days. The flux-based PRA formulation takes into account changes in the uptake kinetics due to changes in the chemical gas-phase and particle surface compositions. This dynamic uptake coefficient approach is employed for the first time in a broader atmospheric context of an urban plume scenario. Our model scenarios include one to three adsorbents and three to five coupled surface reactions. The results show a variation of the O 3 and NO 2 uptake coefficients of more than five orders of magnitude over the course of the simulation time and a decrease in the uptake coefficients in the various scenarios by more than three orders of magnitude within the first six hours. Thereafter, periodic peaks of the uptake coefficients follow the diurnal cycle of gas-phase O 3-NO x reactions. Physisorption of water vapor reduces the half-life of the coating substance BaP by up to a factor of seven by permanently occupying ∼75% of the soot surface. Soot emissions modeled by replenishing reac-tive surface sites lead to maximum gas-phase O 3 depletions of 41 ppbv and 7.8 ppbv for an hourly and six-hourly replenishment cycle, respectively. This conceptual study highlights the interdependence of co-adsorbing species and their non-linear gas-phase feedback. It yields further insight into the atmospheric importance of the chemical oxidation of particles and emphasizes the necessity to implement detailed heterogeneous kinetics in future modeling studies.},
   author = {M Springmann and D A Knopf and N Riemer},
   journal = {Atmos. Chem. Phys},
   pages = {7461-7479},
   title = {Atmospheric Chemistry and Physics Detailed heterogeneous chemistry in an urban plume box model: reversible co-adsorption of O 3 , NO 2 , and H 2 O on soot coated with benzo[a]pyrene},
   volume = {9},
   url = {www.atmos-chem-phys.net/9/7461/2009/},
   year = {2009},
}
@generic{Glasius2016,
   abstract = {Earth's atmosphere contains a multitude of organic compounds, which differ by orders of magnitude regarding fundamental properties such as volatility, reactivity, and propensity to form cloud droplets, affecting their impact on global climate and human health. Despite recent major research efforts and advances, there are still substantial gaps in understanding of atmospheric organic chemistry, hampering efforts to understand, model, and mitigate environmental problems such as aerosol formation in both polluted urban and more pristine regions. The analytical toolbox available for chemists to study atmospheric organic components has expanded considerably during the past decade, opening new windows into speciation, time resolution and detection of reactive and semivolatile compounds at low concentrations. This has provided unprecedented opportunities, but also unveiled new scientific challenges. Specific groundbreaking examples include the role of epoxides in aerosol formation especially from isoprene, the importance of highly oxidized, reactive organics in air-surface processes (whether atmosphere-biosphere exchange or aerosols), as well as the extent of interactions of anthropogenic and biogenic emissions and the resulting impact on atmospheric organic chemistry.},
   author = {Marianne Glasius and Allen H. Goldstein},
   doi = {10.1021/acs.est.5b05105},
   issn = {15205851},
   issue = {6},
   journal = {Environmental Science and Technology},
   month = {3},
   pages = {2754-2764},
   publisher = {American Chemical Society},
   title = {Recent Discoveries and Future Challenges in Atmospheric Organic Chemistry},
   volume = {50},
   year = {2016},
}
@article{Sander2005,
   author = {R Sander and A Kerkweg and P Jöckel and J Lelieveld},
   title = {The new comprehensive atmospheric chemistry module MECCA},
   url = {https://hal.archives-ouvertes.fr/hal-00295611/},
   year = {2005},
}
@article{Wang2016,
   abstract = {Disequilibrium species have been used previously to probe the deep water abundances and the eddy diffusion coefficient for giant planets. In this paper, we present a diffusion-kinetics code that predicts the abundances of disequilibrium species in the tropospheres of Jupiter and Saturn with updated thermodynamic and kinetic data. The dependence on the deep water abundance and the eddy diffusion coefficient is investigated. We quantified the disagreements in CO kinetics that comes from using different reaction networks and identified C2H6 as a useful tracer for the eddy diffusion coefficient. We first apply an H/P/O reaction network to Jupiter and Saturn's atmospheres and suggest a new PH3 destruction pathway. New chemical pathways for SiH4 and GeH4 destruction are also suggested, and another AsH3 destruction pathway is investigated thanks to new thermodynamic and kinetic data. These new models should enhance the interpretation of the measurement of disequilibrium species by JIRAM on board Juno and allow disentangling between methods for constraining the Saturn's deep water abundance with the Saturn entry probes envisaged by NASA or ESA.},
   author = {Dong Wang and Jonathan I. Lunine and Olivier Mousis},
   doi = {10.1016/j.icarus.2016.04.027},
   issn = {10902643},
   journal = {Icarus},
   keywords = {Abundances, atmospheres,Atmospheres, chemistry,Jupiter, atmosphere,Saturn, atmosphere},
   month = {9},
   pages = {21-38},
   publisher = {Academic Press Inc.},
   title = {Modeling the disequilibrium species for Jupiter and Saturn: Implications for Juno and Saturn entry probe},
   volume = {276},
   year = {2016},
}
@article{,
   abstract = {This investigation analysed the growing impact of commercial aviation on CO2 emissions, as well as its potential impact on climate change. It reviewed the effects of the Japanese Aviation Fuel Tax (koukuukinenryouzei), which has been levied on fuel loaded into all domestic flights in Japan since 1972. Using a Bayesian structural time series model, based on monthly observations of fuel consumption between 2004 and 2013 provided by the Ministry of Land, Transport, Infrastructure and Tourism - Japan, this research estimated the effect that this tax has had on the national demand for aviation fuel. It was established that the fuel tax has unequivocally reduced the amount of CO2 emissions from aircraft.},
   author = {Rodrigo González and Eiji B. Hosoda},
   doi = {10.1016/j.jairtraman.2016.08.006},
   issn = {09696997},
   journal = {Journal of Air Transport Management},
   keywords = {Aircraft emissions,Aviation fuel tax,Environmental tax,Global warming,Jet fuel},
   month = {10},
   pages = {234-240},
   publisher = {Elsevier Ltd},
   title = {Environmental impact of aircraft emissions and aviation fuel tax in Japan},
   volume = {57},
   year = {2016},
}
@web_page{,
   title = {MAGMA},
   url = {https://icl.cs.utk.edu/magma/},
}
@report{,
   abstract = {Since the first idea of using GPU to general purpose computing, things have evolved over the years and now there are several approaches to GPU programming. GPU computing practically began with the introduction of CUDA (Compute Unified Device Architecture) by NVIDIA and Stream by AMD. These are APIs designed by the GPU vendors to be used together with the hardware that they provide. A new emerging standard, OpenCL (Open Computing Language) tries to unify different GPU general computing API implementations and provides a framework for writing programs executed across heterogeneous platforms consisting of both CPUs and GPUs. OpenCL provides parallel computing using task-based and data-based parallelism. In this paper we will focus on the CUDA parallel computing architecture and programming model introduced by NVIDIA. We will present the benefits of the CUDA programming model. We will also compare the two main approaches, CUDA and AMD APP (STREAM) and the new framwork, OpenCL that tries to unify the GPGPU computing models. Introduction Parallel computing offer a great advantage in terms of performance for very large applications in different areas like engineering, physics, biology, chemistry, computer vision, econometrics. Since the first supercomputers in early '70s, the nature of parallel computing has changed and new opportunities and challenges have appeared over the time. While 30-35 years ago computer scientists used massively parallel processors like the Goodyear MPP 1 Connection Machine 2 , Ultracomputer 3 and machines using Transputers 4 or dedicated parallel vector computers, like Cray computer series, nowadays off-the-shelves desktops have FLOP rates greater than a supercomputer in late 80's. For example, if Cray 1 had a peak performance of 80 Megaflops and CRAY X MP had a peak performance of 200 MFLOPS an actual multicore Intel processor has more than 120 GFLOPS. These figures emphasize a major shift in computer and processors design. The processors speed increased as the clock frequency and the number of transistors increased over the time. The clock frequency has increased by almost four orders of magnitudes between the first 8088 Intel processor and the actual processors, and the number of transistors also raised from 29.000 for Intel 8086 to approximately 730 million for an Intel Core i7-920 processor 5. The increased clock frequency has an important side effect-the heat dissipated by processors. To overcome this problem, instead of increasing the clock frequency the processor designers come with a new},
   author = {Bogdan Oancea and Tudorel Andrei and Raluca Mariana Dragoescu},
   keywords = {CUDA,GPU computing,OpenCL,Stream,parallel computing},
   title = {GPGPU COMPUTING},
}
@web_page{,
   title = {Cantera},
   url = {https://cantera.org/},
}
@article{Lapointe2019,
   abstract = {Sparse, iterative simulation methods for one-dimensional laminar flames are proposed. The resulting steady and unsteady flame solvers exploit approximate Jacobians to greatly reduce the computational cost associated with matrix operations. The constant, non-unity Lewis number assumption is introduced to further reduce the computational cost. The solvers are also parallelized to reduce time-to-solution on distributed memory computer systems. Computed laminar flame speeds and species profiles for a range of chemical mechanisms (from 10 to 2878 species) are compared against a well-validated commercial code and are found to be consistent within solver tolerances. The computation times of both the unsteady and steady solutions increase only linearly with the number of species, which is a significant improvement over the quadratic or cubic scaling of existing steady-state flame solvers. For the largest mechanism tested, the steady-state flame solver is two orders of magnitude faster than commonly-used codes. The use of an approximate Jacobian is shown to reduce the rate of convergence for the steady-state solver, but does not significantly affect the domain of convergence. The steady-state solver with approximate Jacobian is thus well suited for computationally efficient laminar flame speed sweeps with large kinetic mechanisms.},
   author = {Simon Lapointe and Russell A. Whitesides and Matthew J. McNenly},
   doi = {10.1016/j.combustflame.2019.02.030},
   issn = {15562921},
   journal = {Combustion and Flame},
   keywords = {Approximate factorization,Chemical kinetics,Laminar flame speed,Preconditioner},
   month = {6},
   pages = {23-32},
   publisher = {Elsevier Inc.},
   title = {Sparse, iterative simulation methods for one-dimensional laminar flames},
   volume = {204},
   year = {2019},
}
@article{MacArt2016,
   abstract = {Two formally second-order accurate, semi-implicit, iterative methods for the solution of scalar transport–reaction equations are developed for Direct Numerical Simulation (DNS) of low Mach number turbulent reacting flows. The first is a monolithic scheme based on a linearly implicit midpoint method utilizing an approximately factorized exact Jacobian of the transport and reaction operators. The second is an operator splitting scheme based on the Strang splitting approach. The accuracy properties of these schemes, as well as their stability, cost, and the effect of chemical mechanism size on relative performance, are assessed in two one-dimensional test configurations comprising an unsteady premixed flame and an unsteady nonpremixed ignition, which have substantially different Damköhler numbers and relative stiffness of transport to chemistry. All schemes demonstrate their formal order of accuracy in the fully-coupled convergence tests. Compared to a (non-)factorized scheme with a diagonal approximation to the chemical Jacobian, the monolithic, factorized scheme using the exact chemical Jacobian is shown to be both more stable and more economical. This is due to an improved convergence rate of the iterative procedure, and the difference between the two schemes in convergence rate grows as the time step increases. The stability properties of the Strang splitting scheme are demonstrated to outpace those of Lie splitting and monolithic schemes in simulations at high Damköhler number; however, in this regime, the monolithic scheme using the approximately factorized exact Jacobian is found to be the most economical at practical CFL numbers. The performance of the schemes is further evaluated in a simulation of a three-dimensional, spatially evolving, turbulent nonpremixed planar jet flame.},
   author = {Jonathan F. MacArt and Michael E. Mueller},
   doi = {10.1016/j.jcp.2016.09.016},
   issn = {10902716},
   journal = {Journal of Computational Physics},
   keywords = {Chemical Jacobian,Direct numerical simulation,Operator splitting,Strang splitting,Turbulent reacting flows},
   month = {12},
   pages = {569-595},
   publisher = {Academic Press Inc.},
   title = {Semi-implicit iterative methods for low Mach number turbulent reacting flows: Operator splitting versus approximate factorization},
   volume = {326},
   year = {2016},
}
@article{MacArt2016,
   abstract = {Two formally second-order accurate, semi-implicit, iterative methods for the solution of scalar transport–reaction equations are developed for Direct Numerical Simulation (DNS) of low Mach number turbulent reacting flows. The first is a monolithic scheme based on a linearly implicit midpoint method utilizing an approximately factorized exact Jacobian of the transport and reaction operators. The second is an operator splitting scheme based on the Strang splitting approach. The accuracy properties of these schemes, as well as their stability, cost, and the effect of chemical mechanism size on relative performance, are assessed in two one-dimensional test configurations comprising an unsteady premixed flame and an unsteady nonpremixed ignition, which have substantially different Damköhler numbers and relative stiffness of transport to chemistry. All schemes demonstrate their formal order of accuracy in the fully-coupled convergence tests. Compared to a (non-)factorized scheme with a diagonal approximation to the chemical Jacobian, the monolithic, factorized scheme using the exact chemical Jacobian is shown to be both more stable and more economical. This is due to an improved convergence rate of the iterative procedure, and the difference between the two schemes in convergence rate grows as the time step increases. The stability properties of the Strang splitting scheme are demonstrated to outpace those of Lie splitting and monolithic schemes in simulations at high Damköhler number; however, in this regime, the monolithic scheme using the approximately factorized exact Jacobian is found to be the most economical at practical CFL numbers. The performance of the schemes is further evaluated in a simulation of a three-dimensional, spatially evolving, turbulent nonpremixed planar jet flame.},
   author = {Jonathan F. MacArt and Michael E. Mueller},
   doi = {10.1016/j.jcp.2016.09.016},
   issn = {10902716},
   journal = {Journal of Computational Physics},
   keywords = {Chemical Jacobian,Direct numerical simulation,Operator splitting,Strang splitting,Turbulent reacting flows},
   title = {Semi-implicit iterative methods for low Mach number turbulent reacting flows: Operator splitting versus approximate factorization},
   year = {2016},
}
@article{McNenly2015,
   abstract = {The adaptive preconditioners developed in this paper substantially reduce the computational cost of integrating large kinetic mechanisms using implicit ordinary differential equation (ODE) solvers. For a well-stirred reactor, the speedup of the new method is an order of magnitude faster than recent approaches based on direct, sparse linear system solvers. Moreover, the new method is up to three orders of magnitude faster than traditional implementations of the ODE solver where the Jacobian information is generated automatically via finite differences, and the factorization relies on standard, dense matrix operations. Unlike mechanism reduction strategies, the adaptive preconditioners do not alter the underlying system of differential equations. Consequently, the new method achieves its performance gains without any loss of accuracy to within the local error controlled by the ODE solver. Such speedup allows higher fidelity mechanism chemistry to be coupled with multi-dimensional fluid dynamics simulations.},
   author = {Matthew J. McNenly and Russell A. Whitesides and Daniel L. Flowers},
   doi = {10.1016/j.proci.2014.05.113},
   issn = {15407489},
   issue = {1},
   journal = {Proceedings of the Combustion Institute},
   keywords = {Chemical kinetics,Ordinary differential equation,Preconditioner,Sparse matrix},
   pages = {581-587},
   publisher = {Elsevier Ltd},
   title = {Faster solvers for large kinetic mechanisms using adaptive preconditioners},
   volume = {35},
   year = {2015},
}
@article{Savard2015,
   abstract = {A semi-implicit preconditioned iterative method is proposed for the time-integration of the stiff chemistry in simulations of unsteady reacting flows, such as turbulent flames, using detailed chemical kinetic mechanisms. Emphasis is placed on the simultaneous treatment of convection, diffusion, and chemistry, without using operator splitting techniques. The preconditioner corresponds to an approximation of the diagonal of the chemical Jacobian. Upon convergence of the sub-iterations, the fully-implicit, second-order time-accurate, Crank-Nicolson formulation is recovered. Performance of the proposed method is tested theoretically and numerically on one-dimensional laminar and three-dimensional high Karlovitz turbulent premixed n-heptane/air flames. The species lifetimes contained in the diagonal preconditioner are found to capture all critical small chemical timescales, such that the largest stable time step size for the simulation of the turbulent flame with the proposed method is limited by the convective CFL, rather than chemistry. The theoretical and numerical stability limits are in good agreement and are independent of the number of sub-iterations. The results indicate that the overall procedure is second-order accurate in time, free of lagging errors, and the cost per iteration is similar to that of an explicit time integration. The theoretical analysis is extended to a wide range of flames (premixed and non-premixed), unburnt conditions, fuels, and chemical mechanisms. In all cases, the proposed method is found (theoretically) to be stable and to provide good convergence rate for the sub-iterations up to a time step size larger than 1 μs. This makes the proposed method ideal for the simulation of turbulent flames.},
   author = {B. Savard and Y. Xuan and B. Bobbitt and G. Blanquart},
   doi = {10.1016/j.jcp.2015.04.018},
   issn = {10902716},
   journal = {Journal of Computational Physics},
   keywords = {Iterative method,Numerical integration,Semi-implicit preconditioning,Stiff chemistry},
   month = {8},
   pages = {740-769},
   publisher = {Academic Press Inc.},
   title = {A computationally-efficient, semi-implicit, iterative method for the time-integration of reacting flows with stiff chemistry},
   volume = {295},
   year = {2015},
}
@article{Hoefler2013,
   abstract = {Hybrid parallel programming with the message passing interface (MPI) for internode communication in conjunction with a shared-memory programming model to manage intranode parallelism has become a dominant approach to scalable parallel programming. While this model provides a great deal of flexibility and performance potential, it saddles programmers with the complexity of utilizing two parallel 123 1122 T. Hoefler et al. programming systems in the same application. We introduce an MPI-integrated shared-memory programming model that is incorporated into MPI through a small extension to the one-sided communication interface. We discuss the integration of this interface with the MPI 3.0 one-sided semantics and describe solutions for providing portable and efficient data sharing, atomic operations, and memory consistency. We describe an implementation of the new interface in the MPICH2 and Open MPI implementations and demonstrate an average performance improvement of 40 % to the communication component of a five-point stencil solver.},
   author = {Torsten Hoefler and James Dinan and Darius Buntinas and Pavan Balaji and Brian Barrett and Ron Brightwell and William Gropp and Vivek Kale and Rajeev Thakur and T Hoefler ETH Zurich and J Dinan and D Buntinas and P Balaji and R Thakur and B Barrett and R Brightwell and W Gropp and V Kale},
   doi = {10.1007/s00607-013-0324-2},
   keywords = {MPI-30 · Shared memory · Hybrid parallel programming Mathematics Subject Classification 68N19 other progamming techniques (objects-oriented,automatic,concurrent,etc),sequential},
   pages = {1121-1136},
   title = {MPI + MPI: a new hybrid approach to parallel programming with MPI plus shared memory},
   volume = {95},
   year = {2013},
}
@report{,
   abstract = {The flux reconstruction (FR) method offers a simple, efficient, and easy to implement method, and it has been shown to equate to a differential approach to discontinuous Galerkin (DG) methods. The FR method is also accurate to an arbitrary order and the isentropic Euler vortex problem is used here to empirically verify this claim. This problem is widely used in computational fluid dynamics (CFD) to verify the accuracy of a given numerical method due to its simplicity and known exact solution at any given time. While verifying our FR solver, multiple obstacles emerged that prevented us from achieving the expected order of accuracy over short and long amounts of simulation time. It was found that these complications stemmed from a few overlooked details in the original problem definition combined with the FR and DG methods achieving high-accuracy with minimal dissipation. This paper is intended to consolidate the many versions of the vortex problem found in literature and to highlight some of the consequences if these overlooked details remain neglected.},
   author = {Seth C Spiegel and H T Huynh and James R Debonis},
   keywords = {CPR,DG,FR,high-order methods},
   title = {A Survey of the Isentropic Euler Vortex Problem using High-Order Methods},
   url = {https://ntrs.nasa.gov/search.jsp?R=20150018403},
}
@report{,
   abstract = {High-performance scientific computing has recently seen a surge of interest in heterogeneous systems, with an emphasis on modern Graphics Processing Units (GPUs). These devices offer tremendous potential for performance and efficiency in important large-scale applications of computational science. However, exploiting this potential can be challenging, as one must adapt to the specialized and rapidly evolving computing environment currently exhibited by GPUs. One way of addressing this challenge is to embrace better techniques and develop tools tailored to their needs. This article presents one simple technique, GPU run-time code generation (RTCG), and PyCUDA, an open-source toolkit that supports this technique. In introducing PyCUDA, this article proposes the combination of a dynamic, high-level scripting language with the massive performance of a GPU as a compelling two-tiered computing platform, potentially offering significant performance and productivity advantages over conventional single-tier, static systems. It is further observed that, compared to competing techniques, the effort required to create codes using run-time code generation with PyCUDA grows more gently in response to growing needs. The concept of RTCG is simple and easily implemented using existing, robust tools. Nonetheless it is powerful enough to support (and encourage) the creation of custom application-specific tools by its users. The premise of the paper is illustrated by a wide range of examples where the technique has been applied with considerable success.},
   author = {Andreas Klöckner and Nicolas Pinto and Yunsup Lee and Bryan Catanzaro and Paul Ivanov and Ahmed Fasih},
   keywords = {Automated Tuning,Code generation,GPU,High-level Languages,Many-core,Massive Parallelism,Single-instruction multiple-data,Software engineering},
   title = {PyCUDA: GPU Run-Time Code Generation for High-Performance Computing},
}
@article{,
   author = {A Klöckner and N Pinto and Y Lee and B Catanzaro and P Ivanov - Parallel Computing and undefined 2012},
   journal = {Elsevier},
   title = {PyCUDA and PyOpenCL: A scripting-based approach to GPU run-time code generation},
   url = {https://www.sciencedirect.com/science/article/pii/S0167819111001281},
}
@book{Bergman2011,
   author = {Theodore L Bergman and Frank P Incropera and David P DeWitt and Adrienne S Lavine},
   isbn = {0-470-50197-9},
   publisher = {John Wiley & Sons},
   title = {Fundamentals of heat and mass transfer},
   year = {2011},
}
@article{Xu2017,
   author = {Yue Xu and Susanna Strada and Nadine Unger and Aihui Wang},
   issn = {1680-7324},
   issue = {22},
   journal = {Atmospheric Chemistry and Physics},
   pages = {13699},
   title = {Future inhibition of ecosystem productivity by increasing wildfire pollution over boreal North America},
   volume = {17},
   year = {2017},
}
@report{Linn1997,
   abstract = {The U.S. Department of Energy's Office of Scientific and Technical Information},
   author = {R R Linn},
   isbn = {LA-13334-T},
   publisher = {Los Alamos National Lab., NM (United States)},
   title = {A transport model for prediction of wildfire behavior},
   url = {https://www.osti.gov/biblio/505313-axyNAo/webviewable/},
   year = {1997},
}
@book{Incropera2007,
   author = {Frank P Incropera and Adrienne S Lavine and Theodore L Bergman and David P DeWitt},
   isbn = {0-471-45728-0},
   publisher = {Wiley},
   title = {Fundamentals of heat and mass transfer},
   year = {2007},
}
@article{Utamura2008,
   author = {Motoaki Utamura and Konstantin Nikitin and Yasuyoshi Kato},
   issn = {1741-6361},
   issue = {1},
   journal = {International Journal of Nuclear Energy Science and Technology},
   pages = {11-31},
   title = {A generalised mean temperature difference method for thermal design of heat exchangers},
   volume = {4},
   year = {2008},
}
@article{Zhao1995,
   author = {Tianshou Zhao and Ping Cheng},
   issn = {0017-9310},
   issue = {16},
   journal = {International Journal of Heat and Mass Transfer},
   pages = {3011-3022},
   title = {A numerical solution of laminar forced convection in a heated pipe subjected to a reciprocating flow},
   volume = {38},
   year = {1995},
}
@article{Skjold2018,
   author = {Trygve Skjold and Claude Souprayen and Sergey Dorofeev},
   doi = {10.1016/j.pecs.2017.09.003},
   issn = {0360-1285},
   journal = {Progress in Energy and Combustion Science},
   keywords = {Explosions,Fires,Knowledge gaps,Loss prevention,Risk assessment,Safety,Scaling},
   pages = {2-3},
   title = {Fires and explosions},
   volume = {64},
   url = {http://www.sciencedirect.com/science/article/pii/S0360128517301508},
   year = {2018},
}
@article{Alhubail2016,
   abstract = {This article investigates the swept rule of space-time domain decomposition, an idea to break the latency barrier via communicating less often when explicitly solving time-dependent PDEs. The swept rule decomposes space and time among computing nodes in ways that exploit the domains of influence and the domain of dependency, making it possible to communicate once per many timesteps without redundant computation. The article presents simple theoretical analysis to the performance of the swept rule which then was shown to be accurate by conducting numerical experiments.},
   author = {Maitham Alhubail and Qiqi Wang},
   doi = {10.1016/j.jcp.2015.11.026},
   issn = {10902716},
   journal = {Journal of Computational Physics},
   keywords = {Domain decomposition,Latency,Numerical solution of PDE,Parallel computing,Space-time decomposition,Swept rule},
   title = {The swept rule for breaking the latency barrier in time advancing PDEs},
   year = {2016},
}
@article{McGrattan2013,
   author = {Kevin McGrattan and Simo Hostikka and Randall McDermott and Jason Floyd and Craig Weinschenk and Kristopher Overholt},
   issue = {6},
   journal = {NIST special publication},
   title = {Fire dynamics simulator user’s guide},
   volume = {1019},
   year = {2013},
}
@article{Ghisu2014,
   author = {Tiziano Ghisu and B Arca and G Pellizzaro and P Duce},
   issue = {1},
   journal = {CMES Computer Modeling in Engineering & Sciences},
   pages = {83-102},
   title = {A level-set algorithm for simulating wildfire spread},
   volume = {102},
   year = {2014},
}
@report{Skamarock2005,
   author = {William C Skamarock and Joseph B Klemp and Jimy Dudhia and David O Gill and Dale M Barker and Wei Wang and Jordan G Powers},
   publisher = {National Center For Atmospheric Research Boulder Co Mesoscale and Microscale …},
   title = {A description of the advanced research WRF version 2},
   year = {2005},
}
@article{Reid2016,
   author = {Colleen E Reid and Michael Brauer and Fay H Johnston and Michael Jerrett and John R Balmes and Catherine T Elliott},
   issn = {0091-6765},
   issue = {9},
   journal = {Environmental Health Perspectives},
   pages = {1334-1343},
   title = {Critical review of health impacts of wildfire smoke exposure},
   volume = {124},
   year = {2016},
}
@article{Magee2018,
   abstract = {The expedient design of precision components in aerospace and other high-tech industries requires simulations of physical phenomena often described by partial differential equations (PDEs) without exact solutions. Modern design problems require simulations with a level of resolution difficult to achieve in reasonable amounts of time—even in effectively parallelized solvers. Though the scale of the problem relative to available computing power is the greatest impediment to accelerating these applications, significant performance gains can be achieved through careful attention to the details of memory communication and access. The swept time–space decomposition rule reduces communication between sub-domains by exhausting the domain of influence before communicating boundary values. Here we present a GPU implementation of the swept rule, which modifies the algorithm for improved performance on this processing architecture by prioritizing use of private (shared) memory, avoiding interblock communication, and overwriting unnecessary values. It shows significant improvement in the execution time of finite-difference solvers for one-dimensional unsteady PDEs, producing speedups of 2–9× for a range of problem sizes, respectively, compared with simple GPU versions and 7–300× compared with parallel CPU versions. However, for a more sophisticated one-dimensional system of equations discretized with a second-order finite-volume scheme, the swept rule performs 1.2–1.9× worse than a standard implementation for all problem sizes.},
   author = {Daniel J Magee and Kyle E Niemeyer},
   doi = {10.1016/j.jcp.2017.12.028},
   issn = {0021-9991},
   journal = {Journal of Computational Physics},
   keywords = {Communication-avoiding algorithms,Computational fluid dynamics,Domain decomposition,GPU computing,High-performance computing,Partial differential equations},
   pages = {338-352},
   title = {Accelerating solutions of one-dimensional unsteady PDEs with GPU-based swept time–space decomposition},
   volume = {357},
   url = {http://www.sciencedirect.com/science/article/pii/S0021999117309221},
   year = {2018},
}
@article{Mandel2011,
   author = {Jan Mandel and Jonathan D Beezley and Adam K Kochanski},
   journal = {arXiv preprint arXiv:1102.1343},
   title = {Coupled atmosphere-wildland fire modeling with WRF-fire},
   year = {2011},
}
@article{Reisner2000,
   author = {Jon Reisner and Shannon Wynne and Len Margolin and Rodman Linn},
   issn = {1520-0493},
   issue = {10},
   journal = {Monthly Weather Review},
   pages = {3683-3691},
   title = {Coupled atmospheric–fire modeling employing the method of averages},
   volume = {128},
   year = {2000},
}
@article{Simmons2012,
   author = {Benjamin M Simmons and Ajay K Agrawal},
   issn = {0010-2202},
   issue = {5},
   journal = {Combustion Science and Technology},
   pages = {660-675},
   title = {Flow blurring atomization for low-emission combustion of liquid biofuels},
   volume = {184},
   year = {2012},
}
@article{Liu2013,
   abstract = {This study investigates trends in wildfire potential in the continental United States under a changing climate. Fire potential is measured by the Keetch–Byram Drought Index (KBDI), which is determined by daily maximum temperature and precipitation. The impact of relative humidity and wind speed is examined by comparing KBDI with the modified Fosberg Fire Weather Index (mFFWI). The present (1971–2000) and future (2041–2070) daily regional climate conditions were obtained by dynamical downscaling of the HadCM3 global projection using HRM3 regional climate model provided by the North America Regional Climate Change Assessment Program (NARCCP). It is shown that fire potential is expected to increase in the Southwest, Rocky Mountains, northern Great Plains, Southeast, and Pacific coast, mainly caused by future warming trends. Most pronounced increases occur in summer and autumn. Fire seasons will become longer in many regions. The future fire potential increase will be less pronounced in the northern Rocky Mountains due to the changes in humidity and wind. Present fire potential is found to have been increasing across continental U.S. in recent decades. The future KBDI increase in the central Plains and the South projected using the HadCM3–HRM3 climate change scenario is smaller than the increases using the climate change scenarios from most of other NARCCAP model combinations. Larger inter-seasonal and inter-annual fire potential variability is expected in the future in the Pacific and Atlantic coastal regions. The projected increases in wildfire potential for many regions of the U.S. suggest that increased resources and management efforts for disaster prevention and recovery would be needed in the future.},
   author = {Yongqiang Liu and Scott L. Goodrick and John A. Stanturf},
   doi = {10.1016/j.foreco.2012.06.049},
   issn = {0378-1127},
   journal = {The Mega-fire reality},
   keywords = {Climate change,Continental U.S.,Keetch–Byram Drought Index (KBDI),Wildfire,modified Fosberg Fire Weather Index (mFFWI)},
   pages = {120-135},
   title = {Future U.S. wildfire potential trends projected using a dynamically downscaled climate change scenario},
   volume = {294},
   url = {http://www.sciencedirect.com/science/article/pii/S037811271200388X},
   year = {2013},
}
@inproceedings{,
   author = {Margherita Di Leo and Daniele de Rigo and Dario Rodriguez-Aseretto and Claudio Bosco and Thomas Petroliagkis and Andrea Camia and Jesús San-Miguel-Ayanz},
   journal = {International Symposium on Environmental Software Systems},
   pages = {11-22},
   publisher = {Springer},
   title = {Dynamic data driven ensemble for wildfire behaviour assessment: a case study},
   year = {2013},
}
@generic{,
   title = {Fire Research Division | NIST},
   url = {https://www.nist.gov/el/fire-research-division-73300},
}
@article{Mell2007,
   abstract = {Physics-based coupled fire–atmosphere models are based on approximations to the governing equations of fluid dynamics, combustion, and the thermal degradation of solid fuel. They require significantly more computational resources than the most commonly used fire spread models, which are semi-empirical or empirical. However, there are a number of fire behaviour problems, of increasing relevance, that are outside the scope of empirical and semi-empirical models. Examples are wildland–urban interface fires, assessing how well fuel treatments work to reduce the intensity of wildland fires, and investigating the mechanisms and conditions underlying blow-up fires and fire spread through heterogeneous fuels. These problems are not amenable to repeatable full-scale field studies. Suitably validated coupled atmosphere–fire models are one way to address these problems. This paper describes the development of a three-dimensional, fully transient, physics-based computer simulation approach for modelling fire spread through surface fuels. Grassland fires were simulated and compared to findings from Australian experiments. Predictions of the head fire spread rate for a range of ambient wind speeds and ignition line-fire lengths compared favourably to experiments. In addition, two specific experimental cases were simulated in order to evaluate how well the model predicts the development of the entire fire perimeter.},
   author = {William Mell and Mary Ann Jenkins and Jim Gould and Phil Cheney},
   issue = {1},
   journal = {International Journal of Wildland Fire},
   keywords = {computational fluid dynamics,fire spread,numerical simulation,wildland fire.},
   pages = {1-22},
   title = {A physics-based approach to modelling grassland fires},
   volume = {16},
   url = {https://doi.org/10.1071/WF06002},
   year = {2007},
}
@article{Smagorinsky1963,
   author = {Joseph Smagorinsky},
   issn = {1520-0493},
   issue = {3},
   journal = {Monthly weather review},
   pages = {99-164},
   title = {General circulation experiments with the primitive equations: I. The basic experiment},
   volume = {91},
   year = {1963},
}
@article{Deardorff1970,
   author = {James W Deardorff},
   issn = {1469-7645},
   issue = {2},
   journal = {Journal of Fluid Mechanics},
   pages = {453-480},
   title = {A numerical study of three-dimensional turbulent channel flow at large Reynolds numbers},
   volume = {41},
   year = {1970},
}
@article{Progias2013,
   abstract = {In this paper, a model based on Cellular Automata (CAs) for predicting wildfire spreading is presented. The proposed model is inspired by existing fire spread models, but also includes a number of changes and additions, compared to them. Primary goal of the paper is to design and implement a software-based model as well as a corresponding hardware-based one that will sufficiently describe real fires, but will also have less stringent requirements on computational resources and computational power for execution. Therefore, an effort has been made to minimize the complexity of the model and the resulting computational burden aiming at an implementation that will have a practical significance in predicting the evolution of a fire. The proposed model is implemented on an Altera®Stratix IV®FPGA (Field-Programmable Gate Array), designed to execute in a parallel way in order to produce valuable information in real time that can be used to optimize response to a fire crisis. The FPGA design results from the automatically produced synthesizable VHDL code of the CA model and is advantageous in terms of low-cost, high speed and portability. The resulting implementation sufficiently depicts the natural phenomenon of wildfire spreading and provides short calculation times. Finally, the presented FPGA implementation of the proposed CA model offers the possibility a portable system to be designed, connected with GPS as well as GIS and/or wind monitoring systems able to provide real-time information concerning the wildfire propagation on the under test area.},
   author = {Pavlos Progias and Georgios Ch. Sirakoulis},
   doi = {10.1016/j.mcm.2012.12.005},
   issn = {0895-7177},
   issue = {5},
   journal = {Mathematical and Computer Modelling},
   keywords = {Cellular automata,FPGA,Modelling,Wildfire spreading},
   pages = {1436-1452},
   title = {An FPGA processor for modelling wildfire spreading},
   volume = {57},
   url = {http://www.sciencedirect.com/science/article/pii/S0895717712003494},
   year = {2013},
}
@report{Alhubail2018,
   abstract = {This article describes a method to accelerate parallel, explicit time integration of two-dimensional unsteady PDEs. The method is motivated by our observation that network latency, not bandwidth or computing power, often limits how fast PDEs can be solved in parallel. The method is called the swept rule of space-time domain decomposition. Compared to conventional, space-only domain decomposition, it communicates similar amount of data, but in fewer messages. The swept rule achieves this by decomposing space and time among computing nodes in ways that exploit the domains of influence and the domain of dependency, making it possible to communicate once per many time steps with no redundant computation. By communicating less often, the swept rule effectively breaks the latency barrier, advancing on average more than one time step per ping-pong latency of the network. The article describes the algorithms, presents simple theoretical analysis to the performance of the swept rule in two spatial dimensions, and supports the analysis with numerical experiments.},
   author = {Maitham Alhubail and Qiqi Wang and John Williams},
   keywords = {Domain decomposition,Numerical solution of PDE,Parallel computing * Corresponding author,Space-time decomposition,Swept rule 2D,latency},
   title = {The swept rule for breaking the latency barrier in time advancing two-dimensional PDEs},
   year = {2018},
}
